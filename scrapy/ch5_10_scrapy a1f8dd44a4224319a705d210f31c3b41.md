# ch5_10_scrapy

## Scrapy

![Untitled](ch5_10_scrapy%20a1f8dd44a4224319a705d210f31c3b41/Untitled.png)

python 비동기 웹 크롤링 프레임워크입니다. 프레임워크를 처음 써보게 되는데, 라이브러리보다 더 강력하게 프로그램의 구조와 틀을 강제하는 도구라고 생각하면 됩니다. 

![Untitled](ch5_10_scrapy%20a1f8dd44a4224319a705d210f31c3b41/Untitled%201.png)

위에 그림은 scrapy 프레임워크의 아키텍쳐입니다. 여기서 우리가 직접 코딩하게 될 부분은 Spiders 부분입니다.spiders만 작성하면 HTTP request를 비동기 적으로 보내는 기능, 동시에 보내는 최대 요청 수 조절 등 웹 크롤링에서 반드시 필요한 기능들을 scrapy가 제공해줍니다. 이런 강력한 기능들을 복잡한 python asyncio 코딩 없이 쉽게 코딩할 수 있

## Scrapy 설치 및 프로젝트 시작하기

먼저 scrapy를 사용하기 위해서 설치를 해보겠습니다.

```bash
pip install scrapy
```

그 다음, scrapy 명령어로 프로젝트를 만듭니다.

```bash
scrapy startproject "프로젝트명"
```

여기서는 naversports라는 이름으로 프로젝트를 만들어보겠습니다. 아래처럼 출력이 나오고 폴더가 생성되면 성공입니다.

![Untitled](ch5_10_scrapy%20a1f8dd44a4224319a705d210f31c3b41/Untitled%202.png)

![Untitled](ch5_10_scrapy%20a1f8dd44a4224319a705d210f31c3b41/Untitled%203.png)

naversports라는 폴더 아래에 여러 python 파일들이 생성되었습니다. 이는 scrapy가 사용하는 파일과 설정 값들입니다. 앞서 아키텍처 다이어그램에서 전체 scrapy web crawler 중에 우리가 짤 부분은 spider 부분이었습니다. scrapy 명령어를 이용해서 spider를 만들어보겠습니다.

```bash
cd naversports
scrapy genspider "spider 이름" "crawling을 시작할 URL"
```

한번 baseball라는 이름으로 spider를 만들어보겠습니다. 시작 URL은 큰 상관 없습니다.

```bash
cd naversports
scrapy genspider baseball "https://sports.news.naver.com"
```

![Untitled](ch5_10_scrapy%20a1f8dd44a4224319a705d210f31c3b41/Untitled%204.png)

크롤러 코드를 작성하기에 앞서서 [util.py](http://util.py) 파일을 만들고, 시작일과 종료일 사이에 날짜들의 문자열 리스트를 리턴해주는 함수를 하나 작성해보겠습니다.

```python
from datetime import datetime, timedelta

def get_dates_between(start_datetime, end_datetime):
    datestr_list = []
    current_datetime = start_datetime

    while current_datetime <= end_datetime:
        datestr_list.append(current_datetime.strftime("%Y%m%d"))
        current_datetime += timedelta(days=1)
    return datestr_list
```

```python
from datetime import datetime

start_datetime = datetime(2022, 1, 1)
end_datetime = datetime(2022, 1, 10)
print(get_dates_between(start_datetime, end_datetime)

>> ['20220101', '20220102', '20220103', '20220104', '20220105', '20220106', '20220107', '20220108', '20220109', '20220110']
```

![Untitled](ch5_10_scrapy%20a1f8dd44a4224319a705d210f31c3b41/Untitled%205.png)

### Scrapy Spider 기본 코딩

이제 날짜 문자열도 쉽게 가져올 수 있으니, 크롤러를 코딩해보겠습니다.

```python
import json
from datetime import datetime

import scrapy

from util import get_dates_between

class BaseballSpider(scrapy.Spider):
    name = "baseball"
    start_urls = ["http://sports.news.naver.com/"]

    def __init__(self):
        start_datetime = datetime(2022, 1, 1)
        end_datetime = datetime(2022, 1, 3)
        self.target_dates = get_dates_between(start_datetime, end_datetime)
        self.article_list_url = "https://sports.news.naver.com/kbaseball/news/list?isphoto=N&date={date}&page={page}"
        self.article_url = "https://sports.news.naver.com/news?oid={oid}&aid={aid}"
 
    def parse(self, response):
        for target_date in self.target_dates:
            target_url = self.article_list_url.format(date=target_date, page=1)
            req = scrapy.Request(
                url=target_url,
                callback=self.parse_total_pages
            )
            req.meta["date"] = target_date
            yield req

    def parse_total_pages(self, response):
        resp_json = json.loads(response.text)
        total_pages = resp_json["totalPages"]
        print("total pages", total_pages)
```

기본적으로 scrapy spider는 scrapy의 Spider 클래스를 상속한 형태를 갖습니다. 클래스 아래에 이름과 start_urls를 넣어줍니다. 이 둘은 scrapy framework가 요구하는 필수 요소입니다.

그 다음 BaseballSpider 클래스의 생성자 함수 안에 데이터를 가져오고자 하는 날짜 목록과 크롤링 시에 사용할 URL 포맷들을 넣어놓습니다.

그 다음으로 parse와 parse_total_pages 함수가 보입니다. 기본적으로 scrapy crawler를 작동시키면 start_urls에 포함되어 있는 url들에 요청을 보내고, 그 요청에 응답이 오면 parse 함수에 response를 실어서 호출합니다. 그러면 parse 함수 내에서 다음 요청 만들고, 그 요청의 callback 함수에 새로 만든 요청에 대한 파싱 로직을 넣어서 yield 명령어로 요청을 보내면 됩니다.

yield 명령어는 이전에 generator를 다룰 때 잠깐 등장했었지만, 깊이 살펴보지는 않겠습니다. (다시 사용할 일이 드물기 때문). 여기서는 비동기 방식으로 return 해준다고 생각하면 됩니다. scrapy 프레임워크의 사용법을 익힌다고 생각하면 됩니다.

이제 크롤러를 한번 동작시켜 보겠습니다.

```python
scrapy crawl baseball
```

![Untitled](ch5_10_scrapy%20a1f8dd44a4224319a705d210f31c3b41/Untitled%206.png)

### [settings.py](http://settings.py) 수정하기

뭔가 후루룩 실행되긴 합니다. 그런데 결과를 보면 robots.txt에 의해서 Forbidden 되었다고 나옵니다. 

![Untitled](ch5_10_scrapy%20a1f8dd44a4224319a705d210f31c3b41/Untitled%207.png)

[settings.py](http://settings.py) 파일을 열어서 ROBOTS_OBEY = False로 변경한 뒤에, scrapy crawl 명령어를 실행시켜 보겠습니다. 그러면 우리가 의도한 대로 API에 요청을 보낸 뒤, 결과를 가져와서 전체 페이지 수를 파싱하여 print 해줍니다.

![Untitled](ch5_10_scrapy%20a1f8dd44a4224319a705d210f31c3b41/Untitled%208.png)

그 다음, 동시에 요청하는 요청의 수도 조정할 수 있습니다. 같은 URL에 동시에 너무 많은 요청을 보내면 차단의 위험성이 있으니, CONCURRENT_REQUEST의 수를 4로 조정하겠습니다.

![Untitled](ch5_10_scrapy%20a1f8dd44a4224319a705d210f31c3b41/Untitled%209.png)

### 스포츠 뉴스 기사 목록 수집하기

전체 페이지를 가져왔으니, 이제 for문을 돌면서 각 페이지에 요청을 보내겠습니다. 그리고 각 리스트 응답에 대해서 oid와 aid를 파싱하는 parse_article_list 함수를 만들고, callback 함수로 지정해보겠습니다.

```python
def parse_total_pages(self, response):
    date = response.meta["date"]
    resp_json = json.loads(response.text)
    total_pages = resp_json["totalPages"]
    for i in range(total_pages):
        req = scrapy.Request(
            url=self.article_list_url.format(date=date, page=i+1),
            callback=self.parse_article_list
        )
        req.meta["date"] = date
        yield req

def parse_article_list(self, response):
    resp_json = json.loads(response.text)
    for item in resp_json["list"]:
        oid = item["oid"]
        aid = item["aid"]
        print(oid, aid)
```

![Untitled](ch5_10_scrapy%20a1f8dd44a4224319a705d210f31c3b41/Untitled%2010.png)

scrapy crawl 명령어를 실행해보면 oid와 aid를 쭉쭉 긁어와서 출력을 해줍니다.

### 스포츠 뉴스 기사 본문 수집하기

이제 oid, aid를 가지고 다시 요청을 보내어 뉴스 기사 본문을 긁어와 보겠습니다. 파싱 로직은 지난 시간에 작성한 코드를 그대로 사용하겠습니다.

```python
def parse_article_list(self, response):
    date = response.meta["date"]
    resp_json = json.loads(response.text)
    for item in resp_json["list"]:
        oid = item["oid"]
        aid = item["aid"]
        req = scrapy.Request(
            url=self.article_url.format(oid=oid, aid=aid),
            callback=self.parse_article
        )
        req.meta["date"] = date
        yield req

def parse_article(self, response):
    date = response.meta["date"]

    def _remove_tags(parent_soup, target_tag):
        tags = parent_soup.find_all(target_tag)
        for tag in tags:
            tag.decompose()

    soup = BeautifulSoup(response.text, "lxml")
    title = soup.find("h4", class_="title").get_text()
    content_soup = soup.find("div", id="newsEndContents")
    _remove_tags(content_soup, "p")
    _remove_tags(content_soup, "div")
    _remove_tags(content_soup, "em")
    _remove_tags(content_soup, "span")
    content = content_soup.get_text().strip()
    print(title, content)
```

![Untitled](ch5_10_scrapy%20a1f8dd44a4224319a705d210f31c3b41/Untitled%2011.png)

crawler를 동작시켜보면 뉴스 제목과 본문이 쭉쭉 긁어져서 출력되는 것을 볼 수 있습니다.

### 결과를 CSV 파일에 담기

특정 일의 뉴스 기사 페이지 수를 요청하고, 뉴스 기사 페이지에서 oid와 aid를 파싱해서 다시 뉴스 기사 페이지를 요청하고, 뉴스 기사 페이지를 파싱해서 제목과 본문을 가져왔습니다. 이제 결과를 csv 파일에 담아보겠습니다. 다른 방법들도 많이 있습니다만, 여기서는 Spider 클래스의 멤버 변수에 데이터를 출력할 파일와 csv writer를 만들어서 사용하겠습니다.

```python
def __init__(self):
	...
	self.output_file = open("./baseball.csv", "w")
  self.writer = csv.writer(self.output_file)

def parse_article(self, response):
    ...
    self.output_writer.writerow([date, title, content])
```

이제 start_datetime을 2022년 1월 1일, end_datetime을 2022년 1월 3일로 놓고 scrapy crawl 명령어를 수행해보겠습니다. 약 200개의 HTTP 요청을 11초만에 처리하는 모습을 보여줍니다. 또한 전체 요청의 개수와 성공한 요청의 개수 등의 통계값들을 보여줍니다. 동시 요청 수를 높이면 더 빨리 완수했겠지만, 이정도로 만족하겠습니다. 

![Untitled](ch5_10_scrapy%20a1f8dd44a4224319a705d210f31c3b41/Untitled%2012.png)

결과 csv 파일을 확인해보면 뉴스 기사의 제목과 본문을 제대로 수집한 것을 볼 수 있습니다.

![Untitled](ch5_10_scrapy%20a1f8dd44a4224319a705d210f31c3b41/Untitled%2013.png)

### 본격 크롤링 돌리기

이제 start_datetime을 2022년 1월 1일, end_datetime을 2022년 12월 31일로 놓고 크롤링을 돌려보겠습니다. 총 10만개의 기사를 수집해오는데 약 90분 정도의 시간이 소요되었습니다.

```python
class BaseballSpider(scrapy.Spider):
	...
	def __init__(self):
		  start_datetime = datetime(2022, 1, 1)
		  end_datetime = datetime(2022, 12, 31)
		  self.target_dates = get_dates_between(start_datetime, end_datetime)
	...
```

```python
# 최종 크롤러 코드
import json
from datetime import datetime

import scrapy

from util import get_dates_between
from bs4 import BeautifulSoup
import csv

class BaseballSpider(scrapy.Spider):
    name = "baseball"
    start_urls = ["http://sports.news.naver.com/"]

    def __init__(self):
        start_datetime = datetime(2022, 1, 1)
        end_datetime = datetime(2022, 12, 31)
        self.target_dates = get_dates_between(start_datetime, end_datetime)
        self.article_list_url = "https://sports.news.naver.com/kbaseball/news/list?isphoto=N&date={date}&page={page}"
        self.article_url = "https://sports.news.naver.com/news?oid={oid}&aid={aid}"
        self.output_file = open("./baseball.csv", "w")
        self.writer = csv.writer(self.output_file)
        self.writer.writerow(["date", "title", "content"])

    def parse(self, response):
        for target_date in self.target_dates:
            target_url = self.article_list_url.format(date=target_date, page=1)
            req = scrapy.Request(
                url=target_url,
                callback=self.parse_total_pages
            )
            req.meta["date"] = target_date
            yield req

    def parse_total_pages(self, response):
        date = response.meta["date"]
        resp_json = json.loads(response.text)
        total_pages = resp_json["totalPages"]
        for i in range(total_pages):
            req = scrapy.Request(
                url=self.article_list_url.format(date=date, page=i+1),
                callback=self.parse_article_list
            )
            req.meta["date"] = date
            yield req

    def parse_article_list(self, response):
        date = response.meta["date"]
        resp_json = json.loads(response.text)
        for item in resp_json["list"]:
            oid = item["oid"]
            aid = item["aid"]
            req = scrapy.Request(
                url=self.article_url.format(oid=oid, aid=aid),
                callback=self.parse_article
            )
            req.meta["date"] = date
            yield req

    def parse_article(self, response):
        date = response.meta["date"]

        def _remove_tags(parent_soup, target_tag):
            tags = parent_soup.find_all(target_tag)
            for tag in tags:
                tag.decompose()

        soup = BeautifulSoup(response.text, "lxml")
        title = soup.find("h4", class_="title").get_text()
        content_soup = soup.find("div", id="newsEndContents")
        _remove_tags(content_soup, "p")
        _remove_tags(content_soup, "div")
        _remove_tags(content_soup, "em")
        _remove_tags(content_soup, "span")
        content = content_soup.get_text().strip()
        self.writer.writerow([date, title, content])
```

최종 크롤러 코드를 보면 크롤링 진행 순서에 맞게 파싱 로직들이 분리되어 짜여져 있어서 흐름 파악이 쉽습니다. 또한  동시에 보내는 HTTP 요청 수를 지정하기 쉬웠으며, 결과를 파일에 저장하기도 쉬웠습니다. 이 같은 장점들 때문에 대량의 데이터를 수집해야하는 상황에서는 밑바닦부터 직접 크롤러를 짜지 않고, scrapy 크롤러를 이용하는 것이 좋습니다.

## 정리

이번 장에서는 python web crawling framework인 Scrapy의 기본 동작 원리를 살펴보고, 기존에 requests를 이용해 작성했던 크롤러를 scrapy 방식으로 옮겨보았습니다. 그리고 scrapy를 이용한 고성능 크롤러로 2022년 한해동안의 한국 야구 스포츠 뉴스 10만건을 수집해보았습니다.
