{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yeomko22/data_analytics_camp_2023_share/blob/main/week18_deeplearning_cv/ch18_5_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhrBA5IYi2Ph"
   },
   "source": [
    "# ch 5. fine-tuning\n",
    "\n",
    "참고자료: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDO2-YK_jBoK"
   },
   "source": [
    "fine-tuning은 대량의 데이터에 대해서 이미 학습된 모델을 가져와 특정한 목적에 맞게끔 조금만 학습시키는 기법을 말합니다. CNN 모델의 경우, 이미지 넷으로 학습시킨 모델을 가져오게 되면 모델이 이미 이미지의 선이나 색상 등 low-level feature들을 학습한 상태이므로, 뒷 단의 레이어만 튜닝을 해주면 적은 데이터로도 성능이 뛰어난 모델을 얻을 수 있습니다. fine-tuning에도 여러 전략이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHKYv3X9jIF5"
   },
   "source": [
    "![test](https://storage.googleapis.com/data-analytics-camp/week11_deeplearning_cv/ch5/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4PDVJzajZa1"
   },
   "source": [
    "Strategy 1은 pretrained 모델을 가져와 전체 레이어들을 fine-tuning하는 방법입니다. 이는 데이터 셋이 충분히 많을때 유용하지만, 잘 사용하지 않는 방법입니다.\n",
    "\n",
    "Strategy 2는 일부 Convolution Layer와 마지막에 Fully Connected Layer를 학습시키는 방식을 말합니다. 이는 pretrained model이 학습한 데이터 셋과 내 데이터 셋이 다를 경우 선택하는 방법입니다.\n",
    "\n",
    "Strategy 3은 마지막 Fully Connected Layer만 학습시키는 방식을 말합니다. 보통 Fine-tuning이라고 하면 가장 먼저 이 방식을 적용해보고, 성능이 만족스럽지 않을 경우 뒷 단의 Convolution Layer를 조금씩 포함시켜서 다시 학습을 돌려봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VT5ogeIQMizI"
   },
   "source": [
    "## 사전 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FU0woAgXD1_V"
   },
   "source": [
    "### 디바이스 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2VJiYlujbRo"
   },
   "source": [
    "### 데이터 셋 준비\n",
    "\n",
    "먼저 실습을 진행할 데이터 셋을 준비하겠습니다. 먼저 200장 남짓의 개미와 벌 이미지 데이터 셋이 주어졌고, 이 둘을 분류하는 모델을 만들어야 한다고 가정하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaeBsq2dHdSo"
   },
   "source": [
    "우리 실습에서는 이미지넷 데이터를 학습한 ResNet18 모델을 fine-tuning할 예정입니다. 입력 이미지의 전처리 로직은 해당 모델에서 가져오겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADiaEI9EHsLr"
   },
   "source": [
    "(3, 224, 224) 크기로 입력 이미지를 Resize한 다음, 정규화를 통해 소수로 변환시켜 준 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noiYlu37kGYv"
   },
   "source": [
    "### 모델 학습 코드 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-p09oweLyRJ"
   },
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGtcsaqyL463"
   },
   "source": [
    "### 모델 준비\n",
    "\n",
    "torch에 내장된 resnet18 모델을 불러와서 마지막 FC layer를 2진 분류에 맞게 교체해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7r3dSM3qK_18"
   },
   "source": [
    "마지막 레이어가 1000개의 클래스로 분류해주도록 설정되어 있습니다. 이를 2개의 클래스만 분류하도록 교체해주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4zP27QikBvk"
   },
   "source": [
    "### Strategy 0. Train from scratch\n",
    "\n",
    "첫 번째는 pretrained weight 없이 모델을 가져와서 처음부터 학습을 시켜보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiVK2NVeJeG1"
   },
   "source": [
    "### Strategy 1. Finetune entire model\n",
    "\n",
    "이번에는 pretrained model을 가져와서 모델 전체를 fine-tuning해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8MS0CaPKDFk"
   },
   "source": [
    "단순히 학습을 시작할 때 웨이트 값을 이미지 넷에 대해서 학습한 웨이트로 교체했을 뿐인데 모델의 성능이 훨씬 뛰어난 것을 확인할 수 있습니다. 바로 이것이 pretrained model을 사용하는 이유입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dPvAz_mKOmO"
   },
   "source": [
    "### Strategy 2. Finetune some convolution layer\n",
    "\n",
    "이번에는 두번째 전략을 시도해보겠습니다. 바로 앞단의 컨볼루션 레이어들은 pretrained weight로 고정하고, 뒷 단의 컨볼루션 레이어와 FC 레이어를 fine-tuning 시켜보겠습니다.\n",
    "\n",
    "뒷단의 레이어만 학습이 가능하도록 설정하려면 먼저 전체 모델의 파라미터들을 freeze 해주고, 학습을 적용할 레이어만 freeze를 풀어주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xj4HpxMRP9ag"
   },
   "source": [
    "안정적으로 학습이 진행되면서 동시에 가장 뛰어난 성능을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knLM8k_lPwpn"
   },
   "source": [
    "### Strategy 3. Finetune only FC layer\n",
    "\n",
    "마지막 fine-tuning 기법은 마지막 FC 레이어만 학습하는 방법입니다. 이는 전체 모델 weight는 고정한 채로 FC layer만 새로 교체한 뒤에 학습을 해주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOyQ8OttQu5r"
   },
   "source": [
    "마지막 FC 레이어만 학습을 시키니 학습이 빠르고 안정적으로 진행됩니다. 여기에 성능까지 가장 뛰어난 모습을 보여줍니다. 이처럼 데이터가 부족한 상황에서는 pretrained model의 weight를 고정한 채로 마지막 레이어만 fine-tuning하는 기법이 상당히 유용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYK6YFA0R1tG"
   },
   "source": [
    "## 모델 저장하기\n",
    "\n",
    "학습을 완료한 모델은 따로 저장해보겠습니다. pytorch 모델 확장자는 보통 pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMxfRDPKREF4"
   },
   "source": [
    "## 정리\n",
    "\n",
    "이번 챕터에서는 pretrained model을 가져와서 우리 데이터 셋에 맞게끔 fine-tuning하는 기법에 대해서 알아보았습니다. fine-tuning을 하게 될 경우, 빠르고 안정적으로 모델 학습을 할 수 있을 뿐만아니라 성능까지 뛰어났습니다. 실제로 fine-tuning은 실무에서 많이 활용되니, 사용법을 잘 익혀주시기 바랍니다."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOFlgmhxQHigQ26pt/m0Ur7",
   "gpuType": "T4",
   "include_colab_link": true,
   "mount_file_id": "https://github.com/yeomko22/data_analytics_camp_2023_share/blob/main/week18_deeplearning_cv/ch18_5_fine_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
