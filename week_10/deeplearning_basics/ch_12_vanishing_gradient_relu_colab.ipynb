{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeomko22/data_analytics_camp_6th/blob/main/week_10/deeplearning_basics/ch_12_vanishing_gradient_relu_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ch_12 Vanishing Gradient, ReLU\n",
        "\n",
        "이전 챕터에서 여러 층으로 구성된 Multi Layer Perceptron 모델을 만들고, 분류 모델을 학습시켜 보았습니다. 여기서 궁금해집니다. 여러 층을 쌓을 수록 성능이 좋아진다면 무한히 층을 여러개 쌓으면 성능이 계속해서 좋아지지 않을까요? 한번 만들어보겠습니다."
      ],
      "metadata": {
        "id": "G0sGMkQiAgeo"
      },
      "id": "G0sGMkQiAgeo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://storage.googleapis.com/data-analytics-camp/week10_deeplearning_basics/1.png)"
      ],
      "metadata": {
        "id": "1D__T_6HAimt"
      },
      "id": "1D__T_6HAimt"
    },
    {
      "cell_type": "markdown",
      "id": "5171795e-0077-472d-8a12-5b84ad28ecc3",
      "metadata": {
        "id": "5171795e-0077-472d-8a12-5b84ad28ecc3"
      },
      "source": [
        "## 데이터 셋 준비"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vUgRut6LCPr-"
      },
      "id": "vUgRut6LCPr-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2b43c08a-fa1d-436d-b0bf-35bf5404dc6c",
      "metadata": {
        "id": "2b43c08a-fa1d-436d-b0bf-35bf5404dc6c"
      },
      "source": [
        "## 깊은 신경망 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d99d5afa-a21a-4df7-9a63-1a1a2609fd3d",
      "metadata": {
        "id": "d99d5afa-a21a-4df7-9a63-1a1a2609fd3d"
      },
      "source": [
        "### 모델 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb4e373-c0c2-4e2f-9b3f-e35eb3f35db5",
      "metadata": {
        "id": "9fb4e373-c0c2-4e2f-9b3f-e35eb3f35db5"
      },
      "source": [
        "총 10층으로 구성된 깊은 신경망 모델을 모델을 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3a5r6UUPCRbx"
      },
      "id": "3a5r6UUPCRbx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "655c3d59-70bf-4da1-befd-e66a0e46600d",
      "metadata": {
        "id": "655c3d59-70bf-4da1-befd-e66a0e46600d"
      },
      "source": [
        "### 하이퍼 파라미터 셋팅"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oy9iAplaCSRC"
      },
      "id": "Oy9iAplaCSRC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8372cadd-9040-4dd3-95f5-dda57ed21ca3",
      "metadata": {
        "id": "8372cadd-9040-4dd3-95f5-dda57ed21ca3"
      },
      "source": [
        "### 학습"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5IfIqUuDCUr6"
      },
      "id": "5IfIqUuDCUr6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d8488d7d-26eb-4661-a500-eacbc98e4f24",
      "metadata": {
        "id": "d8488d7d-26eb-4661-a500-eacbc98e4f24"
      },
      "source": [
        "## Vanishing Gradient\n",
        "\n",
        "층을 더 깊게 쌓을 수록 오히려 모델이 학습되지 않는 현상이 발생합니다. 이는 출력 레이어로부터 멀어질 수록, 오류 역전파가 제대로 전달되지 않기 때문입니다. 이런 현상이 바로 두번째 AI winter를 불러온 vanishing gradient 현상입니다.\n",
        "\n",
        "### Sigmoid\n",
        "앞서서 뉴럴 넷 한층을 통과한 값들을 sigmoid에 통과시킨 뒤, 다음 레이어로 전달했습니다. 이 때, sigmoid 함수 때문에 vanishing gradient 현상이 발생합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://storage.googleapis.com/data-analytics-camp/week10_deeplearning_basics/2.png)"
      ],
      "metadata": {
        "id": "Fp3N9uTxAFsT"
      },
      "id": "Fp3N9uTxAFsT"
    },
    {
      "cell_type": "markdown",
      "id": "63973457-efe9-4f69-8538-47cc6e580538",
      "metadata": {
        "id": "63973457-efe9-4f69-8538-47cc6e580538"
      },
      "source": [
        "sigmoid 함수와 sigmoid 함수를 미분한 함수의 그래프를 그려보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://storage.googleapis.com/data-analytics-camp/week10_deeplearning_basics/3.png)"
      ],
      "metadata": {
        "id": "nJJ4vfuuAQHu"
      },
      "id": "nJJ4vfuuAQHu"
    },
    {
      "cell_type": "markdown",
      "id": "93df951a-56d8-4900-b984-3610be79e104",
      "metadata": {
        "id": "93df951a-56d8-4900-b984-3610be79e104"
      },
      "source": [
        "sigmoid 함수는 0과 1 사이로 출력값을 줍니다. 이를 미분한 함수는 최대 값이 0.5인 완만한 함수입니다. 즉, back propagation을 하기 위해서 각 웨이트 별로 chain rule을 통해서 편미분 값을 계산하면, sigmoid 층을 한번 통과할 때마다 편미분 값이 0.5 이하로 줄어든다는 의미입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://storage.googleapis.com/data-analytics-camp/week10_deeplearning_basics/4.png)"
      ],
      "metadata": {
        "id": "YMFgJ1mqAQl2"
      },
      "id": "YMFgJ1mqAQl2"
    },
    {
      "cell_type": "markdown",
      "id": "216d9d45-1546-431a-9ee1-84ee0c1fcd92",
      "metadata": {
        "id": "216d9d45-1546-431a-9ee1-84ee0c1fcd92"
      },
      "source": [
        "역전파를 하기 위해 앞쪽에 위치한 웨이트 값들의 편미분 값을 계산하면 sigmoid를 미분한 함수를 통과하면서 값이 점점 작아져 0에 수렴하게 됩니다. 즉, 층을 더 깊이 쌓을 수록 앞쪽에 위치한 레이어들은 학습이 이루어지지 않습니다. 이를 gradient가 점점 사라진다고 하여 vanishing gradient 라고 부릅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7db68be0-155b-4106-8c82-06bc54741046",
      "metadata": {
        "id": "7db68be0-155b-4106-8c82-06bc54741046"
      },
      "source": [
        "### ReLU\n",
        "\n",
        "그래서 등장한 것이 ReLU 함수입니다.  ReLU 함수는 입력 값이 0보다 작을 때는 0, 0보다 크면 값을 그대로 리턴해주는 함수입니다. 이를 미분한 함수는 0이하일 때는 0, 0보다 크면 1을 리턴하는 함수가 됩니다. 앞서서 깊이 쌓은 신경망 모델의 activation function을 ReLU 함수로 교체해보면 아래와 같습니다.\n",
        "\n",
        "![스크린샷 2023-07-14 오전 11.51.34.png](https://storage.googleapis.com/data-analytics-camp/week10_deeplearning_basics/5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cfc419b-51dd-48b6-943e-49a9acfa5f6b",
      "metadata": {
        "id": "7cfc419b-51dd-48b6-943e-49a9acfa5f6b"
      },
      "source": [
        "## ReLU로 교체 후 재학습"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25cb1e18-d04f-4946-8835-16cfda323d5b",
      "metadata": {
        "id": "25cb1e18-d04f-4946-8835-16cfda323d5b"
      },
      "source": [
        "기존에 sigmoid activation function을 ReLU로 교체한 뒤, 모델을 다시 학습시켜 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yhoO2S6PCX0q"
      },
      "id": "yhoO2S6PCX0q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4e99d3fa-d5a9-469d-9833-b22c5556b883",
      "metadata": {
        "id": "4e99d3fa-d5a9-469d-9833-b22c5556b883"
      },
      "source": [
        "전혀 학습이 이루어지지 않던 상태에서 어느 정도 학습이 진행되었습니다. 바로 ReLU activation을 사용해서 Vanishing Gradient 현상을 완화 시켜준 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0fc8d37-5879-4842-9184-14738f378627",
      "metadata": {
        "id": "e0fc8d37-5879-4842-9184-14738f378627"
      },
      "source": [
        "### 그 밖에 vanishing gradient 극복 방안\n",
        "\n",
        "그럼에도 불구하고  신경망의 층이 깊어지면 vanishing gradient 현상으로 학습이 어려워집니다. ReLU 활성화 함수 이외에도 vanishing gradient 문제를 해결하기 위해 많은 기법들이 제안되었습니다.\n",
        "\n",
        "- weight initialization: 처음 모델을 생성할 때, 웨이트 값 초기화를 잘해서 vanishng gradient를 방지하자는 기법입니다. He initailization이나 Xavier initialization 등이 있습니다.\n",
        "    - 자세한 내용: https://yeomko.tistory.com/40\n",
        "- Batch Normalization: 배치 내에서 데이터를 평균과 분산으로 정규화 하는 기법입니다. 이를 통해 Gradient 전파를 돕고, 학습을 안정화 시킵니다.\n",
        "\n",
        "- Residual Connection: 잔차 연결은 각 계층의 출력을 활성화 함수를 거치지 않고 바로 다음 계층으로 전달하는 구조입니다. 이를 통해 그래디언트가 원본 입력까지 빠르게 전달되어 vanishing gradient 문제를 완화할 수 있습니다. (ResNet의 핵심 아이디어, ResNet는 CS 역사상 가장 많은 citation을 받는 paper)\n",
        "\n",
        "- Gradient Clilpping: 그래디언트의 크기를 임계값 이하로 제한하여 폭주하는 그래디언트를 방지할 수 있습니다.\n",
        "\n",
        "- 다양한 아키텍처: 모델 아키텍처 단에서 vanishing gradient 문제를 완화시키는 방안도 고안되었습니다. InceptionNet 등이 대표적인 사례입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3cbbf58-7518-4e57-83d5-3cbfde231049",
      "metadata": {
        "id": "f3cbbf58-7518-4e57-83d5-3cbfde231049"
      },
      "source": [
        "## 정리\n",
        "\n",
        "이번 챕터에서는 딥러닝 모델 학습이 어려웠던 원인 중 하나인 vanishing gradient에 대해서 알아보았습니다. 그리고 ReLU 활성화 함수를 사용하여 어느정도 극복해보았습니다. Vanishing Gradient 현상의 원인과 극복 방안은 딥러닝 관련 면접 단골 질문들이니 잘 기억해주시기 바랍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03bccee4-cb29-4c0b-9d2c-4b721aca0aac",
      "metadata": {
        "id": "03bccee4-cb29-4c0b-9d2c-4b721aca0aac"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}