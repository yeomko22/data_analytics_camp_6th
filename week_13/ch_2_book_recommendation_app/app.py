import json

import requests
import streamlit as st
from openai import OpenAI
from pinecone import Pinecone

st.set_page_config(
    page_title="worms book",
    page_icon="ğŸ“–",
)


pc = Pinecone(api_key=st.secrets["PINECONE_API_KEY"])
index = pc.Index('books')
openai_client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])


def get_embedding(text_list):
    response = openai_client.embeddings.create(
        input=text_list,
        model="text-embedding-3-small",
        dimensions=512
    )
    return [x.embedding for x in response.data]


def get_translation(query):
    url = "https://asia-northeast3-skilled-chalice-402604.cloudfunctions.net/translate"
    headers = {'Content-Type': 'application/json'}
    payload = json.dumps({"queries": [query]})
    translations = requests.post(
        url=url,
        data=payload,
        headers=headers
    )
    result = translations.json()
    translations = result["translations"]
    return translations[0]


def recommend(query_embedding):
    results = index.query(
        vector=query_embedding,
        top_k=3,
        include_metadata=True,
    )
    matches = results["matches"]
    return [x["metadata"] for x in matches]







def generate_prompt(query, items):
    prompt = f"""
ìœ ì €ê°€ ì½ê³  ì‹¶ì€ ì±…ì— ëŒ€í•œ ë¬˜ì‚¬ì™€ ì´ì— ëŒ€í•œ ì¶”ì²œ ê²°ê³¼ê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.
ìœ ì €ì˜ ì…ë ¥ê³¼ ê° ì¶”ì²œ ê²°ê³¼ ì±…ì˜ ì œëª©, ì €ì, ì†Œê°œ ë“±ì„ ì°¸ê³ í•˜ì—¬ ì¶”ì²œì‚¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
ë‹¹ì‹ ì— ëŒ€í•œ ì†Œê°œë¥¼ ë¨¼ì € í•˜ê³ , ì¹œì ˆí•œ ë§íˆ¬ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì¤‘ê°„ ì¤‘ê°„ ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•´ì£¼ì„¸ìš”.

---
ìœ ì € ì…ë ¥: {query}

ì¶”ì²œ ê²°ê³¼ 1
ì œëª©: {items[0]['title']}
ì €ì: {items[0]['authors']}
ì±…ì†Œê°œ: {items[0]['summary']}

ì¶”ì²œ ê²°ê³¼ 2
ì œëª©: {items[1]['title']}
ì €ì: {items[1]['authors']}
ì±…ì†Œê°œ: {items[1]['summary']}

ì¶”ì²œ ê²°ê³¼ 3
ì œëª©: {items[2]['title']}
ì €ì: {items[2]['authors']}
ì±…ì†Œê°œ: {items[2]['summary']}
---
"""
    return prompt


def request_chat_completion(prompt):
    response = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì±…ì„ ì¶”ì²œí•´ì£¼ëŠ” ì±…ë°©ì§€ê¸°, ì›œì¦ˆì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        stream=True
    )
    return response


def get_author_title(item):
    author = item["authors"]
    title = item["title"]
    author_list = author.split(",")
    if len(author_list) > 1:
        author = f"{author_list[0]} ì™¸ {len(author_list) - 1}ì¸"
    return f"{author} - {title}"


def process_recommend_results(items):
    st.markdown("**ì¶”ì²œê²°ê³¼ ğŸ (ì—´ ìˆ˜ ìˆì–´ìš”!)**")
    for i, item in enumerate(items):
        with st.expander(f"#{i+1} {get_author_title(item)}"):
            st.header(item["title"])
            st.write(f"**{item['authors']}** | {item['publisher']} | {item['published_at']} | [yes24]({item['url']})")
            col1, col2 = st.columns([0.25, 0.75], gap="medium")
            with col1:
                st.image(item["img_url"])
            with col2:
                st.write(item["summary"])


def process_generated_text(streaming_resp):
    st.markdown("**ì›œì¦ˆì˜ ì¶”ì²œì‚¬ âœï¸**")
    report = ""
    res_box = st.empty()
    for chunk in streaming_resp:
        delta = chunk.choices[0].delta
        if delta.content:
            report += delta.content
            res_box.markdown("".join(report).strip())
    return report


st.title("ì›œì¦ˆì˜ ì±…ë°© ğŸ“–ğŸ›")
st.image("./images/banner.png")
with st.form("form"):
    query = st.text_input(
        label="ì½ê³  ì‹¶ì€ ì±…ì„ ë¬˜ì‚¬í•˜ë©´ AIê°€ ì¶”ì²œí•´ì¤ë‹ˆë‹¤ğŸ’¡",
        placeholder="ex) ì¢€ë¹„ì™€ ê°€ì¡±ì• ë¥¼ ë‹¤ë£¬ ì´ì•¼ê¸°"
    )
    submitted = st.form_submit_button("ì œì¶œ")
if submitted:
    if not query:
        st.error("ì½ê³  ì‹¶ì€ ì±… ë¬˜ì‚¬ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”")
    else:
        with st.spinner("ì›œì¦ˆê°€ ì±…ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤..."):
            translated_query = get_translation(query)
            query_embedding = get_embedding(translated_query)
            items = recommend(query_embedding)
        process_recommend_results(items)

        with st.spinner("ì›œì¦ˆê°€ ì¶”ì²œì‚¬ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤..."):
            prompt = generate_prompt(query, items)
            streaming_resp = request_chat_completion(prompt)
        process_generated_text(streaming_resp)
