{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf36e4d-ac8e-4c0c-849b-72348c00c26a",
   "metadata": {},
   "source": [
    "# ch 5. Tokenize\n",
    "자연어 처리에서 텍스트 데이터를 corpus(코퍼스, 말뭉치)라고 부릅니다. 크롤링 시간에 수집했던 뉴스 기사들도 코퍼스라고 부를 수 있습니다. 주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 합니다. \n",
    "\n",
    "토큰은 컴퓨터가 텍스트를 처리하는 최소 의미 단위가 됩니다. 토큰화에는 여러 기법들이 있습니다. 딥러닝 등장 이전의 NLP에서는 주로 언어학 관점에서 정의한 최소 의미 단위인 형태소를 기준으로 토큰화를 했습니다. 하지만 최근 딥러닝 모델들은 데이터로부터 자연스럽게 토크나이저를 학습하는 subword 토큰화를 사용하며, 딥러닝을 이용한 NLP 시간에 배워보겠습니다.\n",
    "\n",
    "형태소 단위로 문장을 토큰화 해주는 기술을 형태소 분석기라고 부릅니다. 형태소 분석기는 언어학적으로 해석이 가능한 장점이 있습니다만, 고유명사나 신조어 대응이 어려운 한계가 있습니다. 이번 챕터에서는 기본적인 토큰화 기법들과 형태소 분석기를 실습해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a16dc3-6cb4-4e9e-9164-c57f1c4a018d",
   "metadata": {},
   "source": [
    "## 공백 기준의 토큰화\n",
    "먼저 생각할 수 있는 가장 단순한 토큰화 기법은 공백을 기준으로 잘라내는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dfdc189-69f8-46ab-b0e2-d510f75edfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '롯데 자이언츠의 거인 이대호(40)의 꿈이 무너져가고 있다. 이대호가 새해 40세 불혹(不惑)의 나이가 됐다. 불혹은 주위에서 어떤 일이 벌어져도 중심을 잃지 않고 자신만의 원칙을 지켜 나갈 수 있는 경지에 오르는 시기다. 이번 스토브리그에 롯데 구단에 걱정스러운 일이 벌어지고 있다. 그런데 이대호는 어떤 대외 활동도 하지 않고 침묵하며 조용히 개인 훈련에 집중하는 모습이다. 불혹이 돼서인가? 지난 2017년 1월24일이다. 벌써 5년의 시간이 흘렀다. 롯데의 프랜차이즈 스타 이대호가 일본프로야구 미국 메이저리그를 거쳐 고향팀 롯데로 돌아왔다. 그는 ‘조선의 4번타자’답게 단숨에 최고가 됐다. 삼성에서 FA가 된 최형우가 고향팀 KIA 타이거즈와 4년 계약을 하면서 기록한 총액 100억 원을 훨씬 넘어 150억 원에 롯데와 계약했다. 당시 인터뷰에서 이대호는 ‘메이저리그에서 열심히 노력해 꿈을 이루었다. 남은 것은 롯데로 돌아와 함께 우승을 하는 것이다. 마지막 소원이 롯데의 우승’이라고 밝혔다. 2001년 롯데에 2차 1순위로 입단해 2011시즌까지 11시즌 동안 이대호는 무려 225개의 홈런을 쏘아 올렸다. 그리고 2008~2011 시즌까지 4년 연속 롯데를 포스트시즌으로 이끌었으나 한국시리즈 우승을 못하고 일본 프로야구로 떠났다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "136eb08a-e1a5-4dc7-85a3-084f7a3c6984",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sentence.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2269eda1-529e-42e4-8540-d3f6ad0bbd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['롯데', '자이언츠의', '거인', '이대호(40)의', '꿈이', '무너져가고', '있다.', '이대호가', '새해', '40세', '불혹(不惑)의', '나이가', '됐다.', '불혹은', '주위에서', '어떤', '일이', '벌어져도', '중심을', '잃지', '않고', '자신만의', '원칙을', '지켜', '나갈', '수', '있는', '경지에', '오르는', '시기다.', '이번', '스토브리그에', '롯데', '구단에', '걱정스러운', '일이', '벌어지고', '있다.', '그런데', '이대호는', '어떤', '대외', '활동도', '하지', '않고', '침묵하며', '조용히', '개인', '훈련에', '집중하는', '모습이다.', '불혹이', '돼서인가?', '지난', '2017년', '1월24일이다.', '벌써', '5년의', '시간이', '흘렀다.', '롯데의', '프랜차이즈', '스타', '이대호가', '일본프로야구', '미국', '메이저리그를', '거쳐', '고향팀', '롯데로', '돌아왔다.', '그는', '‘조선의', '4번타자’답게', '단숨에', '최고가', '됐다.', '삼성에서', 'FA가', '된', '최형우가', '고향팀', 'KIA', '타이거즈와', '4년', '계약을', '하면서', '기록한', '총액', '100억', '원을', '훨씬', '넘어', '150억', '원에', '롯데와', '계약했다.', '당시', '인터뷰에서', '이대호는', '‘메이저리그에서', '열심히', '노력해', '꿈을', '이루었다.', '남은', '것은', '롯데로', '돌아와', '함께', '우승을', '하는', '것이다.', '마지막', '소원이', '롯데의', '우승’이라고', '밝혔다.', '2001년', '롯데에', '2차', '1순위로', '입단해', '2011시즌까지', '11시즌', '동안', '이대호는', '무려', '225개의', '홈런을', '쏘아', '올렸다.', '그리고', '2008~2011', '시즌까지', '4년', '연속', '롯데를', '포스트시즌으로', '이끌었으나', '한국시리즈', '우승을', '못하고', '일본', '프로야구로', '떠났다.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b0328-292d-414b-9a9e-c383419efc5b",
   "metadata": {},
   "source": [
    "이렇게 공백을 기준으로 토큰을 만들게 되면 한국어의 특성상 조사를 떼어낼 수가 없습니다. 즉, \"이대호가\", \"이대호는\"과 같이 실제로는 비슷한 의미를 갖는 토큰을 전혀 다른 토큰으로 인식하게 됩니다. 토큰화가 제대로 이루어지지 않으면 제 아무리 고도화 된 AI 모델을 학습시킨다 하더라도 정확도를 기대할 수 없습니다.\n",
    "\n",
    "공백을 기준으로 잡는 것 외에도 온 점이나 쉼표를 기준으로 토큰화를 하는 기법들이 있지만, 마찬가지로 낮은 정확도로 인해 사용하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c1905d-c9b8-46a4-ad93-f28da0c8b410",
   "metadata": {},
   "source": [
    "## 형태소 분석기를 이용한 토큰화\n",
    "다음으로 형태소 분석기를 이용한 토큰화를 해보겠습니다. 가장 쉽게 사용할 수 있는 konlpy의 komoran을 사용하여 실습을 진행해보겠습니다. konlpy는 한국어 자연어 처리 라이브러리이고, komoran은 konlpy를 통해 이용할 수 있는 형태소 분석기 중 하나입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ba10889-deba-4f38-aa66-9a3dbdf7b700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /Users/user/miniconda3/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /Users/user/miniconda3/lib/python3.10/site-packages (from konlpy) (1.4.1)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /Users/user/miniconda3/lib/python3.10/site-packages (from konlpy) (4.9.2)\n",
      "Requirement already satisfied: numpy>=1.6 in /Users/user/miniconda3/lib/python3.10/site-packages (from konlpy) (1.23.5)\n",
      "Requirement already satisfied: packaging in /Users/user/miniconda3/lib/python3.10/site-packages (from JPype1>=0.7.0->konlpy) (22.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "132c44d9-819b-4df7-8249-4ab080411b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "700173bd-7442-4074-8b49-38ce1d4fe6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6d00aad-cb0c-4df8-beab-319d4ab7cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = komoran.pos(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abd04188-a588-4dfc-87d4-e045420937f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('롯데 자이언츠', 'NNP'), ('의', 'JKG'), ('거인', 'NNP'), ('이대호', 'NNP'), ('(', 'SS'), ('40', 'SN'), (')', 'SS'), ('의', 'JKG'), ('꿈', 'NNG'), ('이', 'JKS'), ('무너지', 'VV'), ('어', 'EC'), ('가', 'VX'), ('고', 'EC'), ('있', 'VX'), ('다', 'EF'), ('.', 'SF'), ('이대호', 'NNP'), ('가', 'JKS'), ('새해', 'NNP'), ('40', 'SN'), ('세', 'NNB'), ('불혹', 'NNG'), ('(', 'SS'), ('不惑', 'SH'), (')', 'SS'), ('의', 'JKG'), ('나이', 'NNG'), ('가', 'JKS'), ('되', 'VV'), ('었', 'EP'), ('다', 'EF'), ('.', 'SF'), ('불혹', 'NNG'), ('은', 'JX'), ('주위', 'NNG'), ('에서', 'JKB'), ('어떤', 'MM'), ('일', 'NNG'), ('이', 'JKS'), ('벌어지', 'VV'), ('어도', 'EC'), ('중심', 'NNG'), ('을', 'JKO'), ('잃', 'VV'), ('지', 'EC'), ('않', 'VX'), ('고', 'EC'), ('자신', 'NNG'), ('만', 'JX'), ('의', 'JKG'), ('원칙', 'NNG'), ('을', 'JKO'), ('지키', 'VV'), ('어', 'EC'), ('나가', 'VV'), ('ㄹ', 'ETM'), ('수', 'NNB'), ('있', 'VV'), ('는', 'ETM'), ('경지', 'NNG'), ('에', 'JKB'), ('오르', 'VV'), ('는', 'ETM'), ('시기', 'NNP'), ('다', 'EF'), ('.', 'SF'), ('이번', 'NNG'), ('스토브리그에', 'NA'), ('롯데', 'NNP'), ('구단', 'NNP'), ('에', 'JKB'), ('걱정', 'NNG'), ('스럽', 'XSA'), ('ㄴ', 'ETM'), ('일', 'NNG'), ('이', 'JKS'), ('벌어지', 'VV'), ('고', 'EC'), ('있', 'VX'), ('다', 'EF'), ('.', 'SF'), ('그런데', 'MAJ'), ('이대호', 'NNP'), ('는', 'JX'), ('어떤', 'MM'), ('대외', 'NNG'), ('활동', 'NNG'), ('도', 'JX'), ('하', 'VV'), ('지', 'EC'), ('않', 'VX'), ('고', 'EC'), ('침묵', 'NNG'), ('하', 'XSV'), ('며', 'EC'), ('조용히', 'MAG'), ('개인', 'NNG'), ('훈련', 'NNG'), ('에', 'JKB'), ('집중', 'NNG'), ('하', 'XSV'), ('는', 'ETM'), ('모습', 'NNG'), ('이', 'VCP'), ('다', 'EF'), ('.', 'SF'), ('불혹', 'NNG'), ('이', 'JKS'), ('되', 'VV'), ('어서', 'EC'), ('이', 'VCP'), ('ㄴ가', 'EF'), ('?', 'SF'), ('지나', 'VV'), ('ㄴ', 'ETM'), ('2017년 1월', 'NNP'), ('24', 'SN'), ('일', 'NNB'), ('이', 'VCP'), ('다', 'EF'), ('.', 'SF'), ('벌써', 'MAG'), ('5년', 'NNP'), ('의', 'JKG'), ('시간', 'NNG'), ('이', 'JKS'), ('흐르', 'VV'), ('었', 'EP'), ('다', 'EF'), ('.', 'SF'), ('롯데', 'NNP'), ('의', 'JKG'), ('프랜차이즈', 'NNP'), ('스타', 'NNP'), ('이대호', 'NNP'), ('가', 'JKS'), ('일본', 'NNP'), ('프로', 'NNP'), ('야구', 'NNP'), ('미국', 'NNP'), ('메이저', 'NNP'), ('리그', 'NNP'), ('를', 'JKO'), ('거치', 'VV'), ('어', 'EC'), ('고향', 'NNG'), ('팀', 'NNG'), ('롯데', 'NNP'), ('로', 'JKB'), ('돌아오', 'VV'), ('았', 'EP'), ('다', 'EF'), ('.', 'SF'), ('그', 'NP'), ('는', 'JX'), ('‘', 'SS'), ('조선', 'NNP'), ('의', 'JKG'), ('4', 'SN'), ('번', 'NNB'), ('타자', 'NNP'), ('’', 'SS'), ('답', 'XSA'), ('게', 'EC'), ('단숨에', 'MAG'), ('최고', 'NNG'), ('가', 'JKS'), ('되', 'VV'), ('었', 'EP'), ('다', 'EF'), ('.', 'SF'), ('삼성', 'NNP'), ('에서', 'JKB'), ('FA', 'SL'), ('가', 'JKS'), ('되', 'VV'), ('ㄴ', 'ETM'), ('최형우', 'NNP'), ('가', 'JKS'), ('고향', 'NNG'), ('팀', 'NNG'), ('KIA 타이거즈', 'NNP'), ('와', 'JC'), ('4', 'SN'), ('년', 'NNB'), ('계약', 'NNG'), ('을', 'JKO'), ('하', 'VV'), ('면서', 'EC'), ('기록', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETM'), ('총액', 'NNG'), ('100', 'SN'), ('억', 'NR'), ('원', 'NNB'), ('을', 'JKO'), ('훨씬', 'MAG'), ('넘', 'VV'), ('어', 'EC'), ('150', 'SN'), ('억', 'NR'), ('원', 'NNB'), ('에', 'JKB'), ('롯데', 'NNP'), ('와', 'JC'), ('계약', 'NNG'), ('하', 'XSV'), ('았', 'EP'), ('다', 'EF'), ('.', 'SF'), ('당시', 'NNG'), ('인터뷰', 'NNP'), ('에서', 'JKB'), ('이대호', 'NNP'), ('는', 'JX'), ('‘', 'SS'), ('메이저', 'NNP'), ('리그', 'NNP'), ('에서', 'JKB'), ('열심히', 'MAG'), ('노력', 'NNG'), ('하', 'XSV'), ('아', 'EC'), ('꿈', 'NNG'), ('을', 'JKO'), ('이루', 'VV'), ('었', 'EP'), ('다', 'EF'), ('.', 'SF'), ('남은', 'NNP'), ('것', 'NNB'), ('은', 'JX'), ('롯데', 'NNP'), ('로', 'JKB'), ('돌아오', 'VV'), ('아', 'EC'), ('함께', 'MAG'), ('우승', 'NNG'), ('을', 'JKO'), ('하', 'VV'), ('는', 'ETM'), ('것', 'NNB'), ('이', 'VCP'), ('다', 'EF'), ('.', 'SF'), ('마지막', 'NNG'), ('소원', 'NNG'), ('이', 'JKS'), ('롯데', 'NNP'), ('의', 'JKG'), ('우승', 'NNP'), ('’', 'SS'), ('이라고', 'JKQ'), ('밝히', 'VV'), ('었', 'EP'), ('다', 'EF'), ('.', 'SF'), ('2001', 'SN'), ('년', 'NNB'), ('롯데', 'NNP'), ('에', 'JKB'), ('2', 'SN'), ('차', 'NNB'), ('1', 'SN'), ('순위', 'NNP'), ('로', 'JKB'), ('입단', 'NNG'), ('하', 'XSV'), ('아', 'EC'), ('2011', 'SN'), ('시즌', 'NNP'), ('까지', 'JX'), ('11', 'SN'), ('시즌', 'NNP'), ('동안', 'NNG'), ('이대호', 'NNP'), ('는', 'JX'), ('무려', 'MAG'), ('225', 'SN'), ('개', 'NNB'), ('의', 'JKG'), ('홈런', 'NNG'), ('을', 'JKO'), ('쏘', 'VV'), ('아', 'EC'), ('올리', 'VV'), ('었', 'EP'), ('다', 'EF'), ('.', 'SF'), ('그리고', 'MAJ'), ('2008', 'SN'), ('~', 'SO'), ('2011', 'SN'), ('시즌', 'NNG'), ('까지', 'JX'), ('4', 'SN'), ('년', 'NNB'), ('연속', 'NNP'), ('롯데', 'NNP'), ('를', 'JKO'), ('포스트', 'NNP'), ('시즌', 'NNP'), ('으로', 'JKB'), ('이끌', 'VV'), ('었', 'EP'), ('으나', 'EC'), ('한국시리즈', 'NNP'), ('우승', 'NNG'), ('을', 'JKO'), ('못하', 'VX'), ('고', 'EC'), ('일본', 'NNP'), ('프로야', 'NNP'), ('구로', 'NNP'), ('떠나', 'VV'), ('았', 'EP'), ('다', 'EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf5895-f091-4152-992e-e5e1a421b510",
   "metadata": {},
   "source": [
    "형태소 분석기를 이용하면 각 토큰별로 품사와 함께 토큰화 된 텍스트를 가져올 수 있습니다. 또한 \"롯데 자이언츠\"처럼 중간에 공백이 삽입된 명사도 고유명사로 잘 구분하는 모습을 보여줍니다. \"이대호가\", \"이대호는\"과 같은 단어도 \"이대호\", \"가\", \"이대호\", \"는\"으로 분리해주어 의미 단위로 토큰을 잘 분리하는 모습을 보여줍니다.\n",
    "\n",
    "하지만 형태소 분석기는 신조어나 고유 명사에 상당히 취약한 모습을 보이기도 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "084dc33a-0d80-43e8-8c76-bb0dba86f91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SSG', 'SL'), ('랜더스의', 'NA'), ('추신수', 'NNP'), ('선수', 'NNG'), ('가', 'JKS'), ('NC 다이노스', 'NNP'), ('의', 'JKG'), ('안우진', 'NNP'), ('선수', 'NNG'), ('를', 'JKO'), ('상대로', 'NNP'), ('홈런', 'NNG'), ('을', 'JKO'), ('치', 'VV'), ('었', 'EP'), ('습니다', 'EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"SSG 랜더스의 추신수 선수가 NC 다이노스의 안우진 선수를 상대로 홈런을 쳤습니다.\"\n",
    "print(komoran.pos(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb5586-69a2-4d83-bb32-05460b0fdc81",
   "metadata": {},
   "source": [
    "\"SSG\", \"랜더스\", \"안우진\" 등의 고유 명사를 NNP로 잡아내지 못하는 모습을 보입니다. 이러한 형태소 분석기의 한계점은 고유 명사를 직접 지정하는 사용자 사전 기능으로 극복할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b46404a4-23b3-4d1c-8801-71b196b00353",
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran = Komoran(userdic=\"./data/user.dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14ce6426-ffbf-4c77-b2a2-c1c8c566af9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SSG', 'NNP'), ('랜더스', 'NNP'), ('의', 'JKG'), ('추신수', 'NNP'), ('선수', 'NNG'), ('가', 'JKS'), ('NC', 'NNP'), ('다이노스', 'NNP'), ('의', 'JKG'), ('안우진', 'NNP'), ('선수', 'NNG'), ('를', 'JKO'), ('상대로', 'NNP'), ('홈런', 'NNG'), ('을', 'JKO'), ('치', 'VV'), ('었', 'EP'), ('습니다', 'EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "print(komoran.pos(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a349110-e507-4a92-86c0-7608ec3c6f89",
   "metadata": {},
   "source": [
    "## 전체 데이터 셋 토크나이즈\n",
    "\n",
    "웹 크롤링 시간에 정규 표현식을 이용해서 전처리 했었던 데이터 셋을 tokenize 해보겠습니다. 한번 전체 데이터 셋 중 1000개만 토큰화하여 결과를 CSV 파일에 써보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ac0a5ff-3e20-41a9-8963-6af11a660683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = []\n",
    "with open(\"./data/baseball_preprocessed.csv\") as fr:\n",
    "    reader = csv.reader(fr)\n",
    "    columns = next(reader)\n",
    "    for row in reader:\n",
    "        data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c914e35c-27af-47b5-9311-8e6f19820641",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49c4aa5d-5f56-4a5c-b888-13c82f50f263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 1000/1000 [00:13<00:00, 74.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"./data/baseball_tokenized.csv\", \"w\") as fw:\n",
    "    writer = csv.writer(fw)\n",
    "    writer.writerow([\"url\", \"datetime_str\", \"title\", \"content\", \"content_tokens\"])\n",
    "    for sample in tqdm(samples):\n",
    "        url, datetime_str, title, content = sample\n",
    "        tokens = komoran.pos(content)\n",
    "        writer.writerow([url, datetime_str, title, content, tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e059a655-a00c-4826-8e5e-5043b599ec64",
   "metadata": {},
   "source": [
    "1000개를 토큰화하는데 13초가 걸렸습니다. 전체 기사는 10만개이므로 모두 토큰화 하는데에는 13 * 100 / 60 = 21분 가량이 소요됩니다. 물론 기다릴 수 있는 시간입니다만 아쉽습니다. 여러개의 CPU를 활용해서 동시에 토큰화 작업을 수행하면 전체 소요 시간이 몇배는 단축되지 않을까요? 다음 챕터에서는 python multiprocessing을 이용해서 멀티 코어로 빠르게 토큰화 작업을 수행하는 방법을 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3cf25-631e-42b3-a97f-26546f17b10c",
   "metadata": {},
   "source": [
    "## 정리\n",
    "이번 챕터에서는 python에서 자연어를 처리하기 위한 첫 스텝인 토큰화에 대해서 알아보았습니다. 그리고 토큰화 방식 중에 하나인 형태소 기반의 토큰화를 알아보았고, konlpy Komoran을 이용해서 토큰화를 해보았습니다.\n",
    "\n",
    "한국어 형태소 분석기에는 Komoran만 있는 것은 아닙니다. 속도 측면에서는 Mecab, 정확도 측면에서는 Khaiii라는 프로젝트가 우수합니다만, 각각 설치 및 사용자 사전 등록이 까다로워서 수업 자료에 사용하지는 않았습니다. 실제 프로젝트를 진행할 때 Komoran의 성능이 아쉽다면 선택해볼만 합니다.\n",
    "\n",
    "다음 챕터에서는 멀티 프로세스를 이용해서 대량의 텍스트를 빠르게 토큰화 하는 방법을 알아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec3e2e7-e2ba-4403-9e70-085bb5ad26a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
