{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e3e1c5-40c2-4a10-963c-56985f4f8dd5",
   "metadata": {},
   "source": [
    "## ch 1. python requests를 이용한 크롤러 개발\n",
    "\n",
    "### 1. 필요 라이브러리\n",
    "이제 본격적으로 크롤러를 개발해보겠습니다. 웹 크롤러를 만들기 위해서는 먼저 http client가 있어야 합니다. 지금까지는 웹 브라우저를 클라이언트로 사용하였는데, python의 reuests라는 라이브러리를 사용하면 쉽게 HTTP 요청을 보낼 수 있습니다. 그리고 응답으로 수신한 HTML 문서를 쉽게 파싱하기 위해서 BeautifulSoup이라는 라이브러리를 이요하겠습니다.\n",
    "\n",
    "- requests\n",
    "- BeautifualSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b111c01-e71a-4ddd-91d3-4f2135d92164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/user/miniconda3/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests) (2022.12.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/user/miniconda3/lib/python3.10/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/user/miniconda3/lib/python3.10/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87231d23-b1e7-4ab5-bc19-b36cb21dff15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d1c2bfb-d04e-466c-b459-590f986364e7",
   "metadata": {},
   "source": [
    "### 2. news list backend API에 HTTP 요청 보내기\n",
    "뉴스 기사 목록을 내려주는 네이버 스포츠 백엔드 API에 2023년 5월 10일 뉴스 기사 목록을 요청을 보낸 뒤, 응답을 json 객체로 바꿔보겠습니다. json은 python에서는 dict 데이터 타입으로 표현됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0898399b-4d74-417e-958f-fee1b5b81610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4965c9ae-f12a-4584-b67a-e033cb422549",
   "metadata": {},
   "source": [
    "### 3. 반목문을 돌면서 모든 페이지의 뉴스 기사 oid, aid 수집하기\n",
    "totalPages 값을 통해서 2023년 5월 10일 야구 뉴스 기사가 총 25 페이지 있다는 것을 알게되었습니다. 우리가 원하는 것은 특정일에 나온 뉴스 기사들의 oid와 aid들입니다. 한번 for문을 돌면서 모든 페이지를 요청하고, oid와 aid를 수집해보겠습니다. \n",
    "\n",
    "전체 진행 상황을 모니터링 하기 위해서 tqdm 라이브러리를 사용해보겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912c408-a9d1-437e-a6dc-b141d630f2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e05215dd-8d2f-43d2-a37a-f4b5b6714bd5",
   "metadata": {},
   "source": [
    "### 4. 뉴스 기사 페이지를 요청한 뒤, 기사 제목과 본문 파싱하기 \n",
    "기사 상세 페이지를 하나만 요청한 뒤, BeautifulSoup을 이용해 파싱해보겠습니다. BeautifulSoup 객체를 만들 때 뒤에 붙여주는 \"lxml\"은 BeautifulSoup에 내장된 HTML 파서 중 가장 많이 사용되는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d0099b-b9b1-4a28-a966-a01b9af01de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8e64cae-6952-405e-a826-1861b7b4939c",
   "metadata": {},
   "source": [
    "기사 본문 외에도 기자 명이나 언론사 링크 등의 불필요한 정보들이 포함되어 있다. 예를 들어 div id=\"newsEndContents\" 태그 안에 포함된 p, div, span, em 태그들은 모두 불필요한 텍스트들을 가지고 있다. 이를 BeautifulSoup를 이용해서 제거한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b7968-618b-4f3f-8d4b-644897dd04f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc693166-6589-4892-91f7-ad4b4423490c",
   "metadata": {},
   "source": [
    "### 5. 코드 정리 및 CSV 파일에 데이터 쓰기\n",
    "지금까지 네이버 스포츠에서 특정 요일에 야구 기사 목록을 가져온 뒤, 각각의 기사의 제목과 본문을 수집하는 크롤러를 개발해보았다. 코드를 정리해보면 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fbddd-23a1-4b4d-ad2b-b58ba5ab9797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
