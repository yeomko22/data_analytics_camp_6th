{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeomko22/data_analytics_camp_6th/blob/main/week_12/deeplearning_nlp/ch_2_huggingface_tokenizers_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b08f5510-1e26-4b95-b62e-22b79a61cd2f",
      "metadata": {
        "id": "b08f5510-1e26-4b95-b62e-22b79a61cd2f"
      },
      "source": [
        "# ch 2. huggingface tokenizers\n",
        "\n",
        "이번 챕터에서는 huggingface에서 제공하는 tokenizers 라이브러리를 이용해서 직접 corpus 데이터 셋을 가지고 subword tokenizer를 학습시켜 보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c6f02dc-81d2-42d6-9431-8adaba8a0a97",
      "metadata": {
        "id": "4c6f02dc-81d2-42d6-9431-8adaba8a0a97"
      },
      "source": [
        "## 데이터 셋 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee9efdff-2f2c-47f2-98b5-f23bbf22acb1",
      "metadata": {
        "id": "ee9efdff-2f2c-47f2-98b5-f23bbf22acb1"
      },
      "source": [
        "### 네이버 영화 리뷰 데이터 셋\n",
        "\n",
        "네이버 영화 리뷰 데이터 셋으로 간단히 학습을 진행해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ph4bfaf3OgHy"
      },
      "id": "Ph4bfaf3OgHy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f4a77185-e75a-416e-8b79-baf5a791b39b",
      "metadata": {
        "id": "f4a77185-e75a-416e-8b79-baf5a791b39b"
      },
      "source": [
        "### 데이터 셋 전처리\n",
        "\n",
        "결측치를 제거하고, 특수문자나 한자를 제거해주겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUqIGsvvOhaW"
      },
      "id": "hUqIGsvvOhaW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bfcfb1b1-092c-417a-b159-2e180b237008",
      "metadata": {
        "id": "bfcfb1b1-092c-417a-b159-2e180b237008"
      },
      "source": [
        "## huggingface tokenizers\n",
        "\n",
        "huggingface는 AI 스타트업으로 오픈 소스 라이브러리로 유명합니다. 특히 NLP 분야에서는 huggingface에서 제공하는 트랜스포머 모델을 사용하는 것이 거의 표준으로 자리잡았습니다. 주요 라이브러리는 아래와 같습니다.\n",
        "\n",
        "- transformers: 트랜스포머 기본 모델과 이를 응용한 NLP 모델들을 제공\n",
        "- tokenizers: subword tokenizer 제공\n",
        "\n",
        "huggingface의 tokenizers는 subword tokenizer의 구현체입니다. 이를 사용하여 tokenizer를 학습시켜 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14fc00b4-059b-44b8-bc50-321f273df38b",
      "metadata": {
        "id": "14fc00b4-059b-44b8-bc50-321f273df38b",
        "outputId": "f881fd0f-cb5e-480e-fa43-4f93b88f471e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in /Users/user/miniconda3/lib/python3.10/site-packages (0.15.0)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /Users/user/miniconda3/lib/python3.10/site-packages (from tokenizers) (0.20.2)\n",
            "Requirement already satisfied: filelock in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.9.2)\n",
            "Requirement already satisfied: requests in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2022.12.7)\n",
            "\u001b[33mDEPRECATION: pytorch-lightning 1.8.3.post1 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: mecab-python 0.996-ko-0.9.2 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of mecab-python or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0212e38a-461e-46d7-8d6b-96d42dc4b9f3",
      "metadata": {
        "id": "0212e38a-461e-46d7-8d6b-96d42dc4b9f3"
      },
      "source": [
        "### Trainer\n",
        "WordPiece 기반의 subword tokenizer를 만들어보겠습니다.  먼저 tokenizer를 학습시키기 위한 trainer를 만들어줍니다. 이 때, special tokens를 넣어주어야 하는데, 각 토큰의 의미는 다음과 같습니다.\n",
        "\n",
        "- [PAD]: padding의 약자로 문장 간에 길이를 맞춰주기 위해 일부러 채워넣은 토큰을 의미합니다.\n",
        "- [UNK]: unknown의 약자로 인식하지 못한 토큰을 나타냅니다.\n",
        "- [SOS]: start of sentence의 약자로 문장의 시작을 표시해줍니다.\n",
        "- [EOS]: end of sentence의 약자로 문장의 끝을 표시해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pGi16PywOjhF"
      },
      "id": "pGi16PywOjhF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c838c82d-a95b-4b0c-a14f-375d3d307577",
      "metadata": {
        "id": "c838c82d-a95b-4b0c-a14f-375d3d307577"
      },
      "source": [
        "### Tokenizer\n",
        "\n",
        "이제 tokenizer 객체를 만들어주고 trainer를 이용해서 학습시켜 줍니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VyrQSZYlOkhm"
      },
      "id": "VyrQSZYlOkhm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e4dd1491-9711-4648-97f2-002325d64b7a",
      "metadata": {
        "id": "e4dd1491-9711-4648-97f2-002325d64b7a"
      },
      "source": [
        "## tokenizer 확인\n",
        "\n",
        "tokenizer가 잘 학습되었는지 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "182ede0e-15e0-4450-99c4-350410db649a",
      "metadata": {
        "id": "182ede0e-15e0-4450-99c4-350410db649a"
      },
      "source": [
        "### vocab 확인\n",
        "\n",
        "먼저 tokenizer vocab에 어떤 토큰들이 추가되었는지 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xmfRPKDCOmDT"
      },
      "id": "xmfRPKDCOmDT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "620f5b96-dbb7-48d2-86b7-5add687a7f21",
      "metadata": {
        "id": "620f5b96-dbb7-48d2-86b7-5add687a7f21"
      },
      "source": [
        "4000 ~ 5000개까지는 초기 기본 토큰들로 채워져 있고, 그 뒤로는 함께 자주 등장하는 글자끼리 묶인 토큰들을 확인할 수 있습니다. 앞에 ##이 붙은 토큰들은 단어의 시작 지점이 아닌 위치에 등장하는 토큰들입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf29bd92-2cbc-4d97-ad0b-bbdaafda9526",
      "metadata": {
        "id": "bf29bd92-2cbc-4d97-ad0b-bbdaafda9526"
      },
      "source": [
        "### 샘플 토큰화\n",
        "\n",
        "예시 문장들을 토큰화 해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9058d6-af61-4e64-9ae5-f2c4493f2910",
      "metadata": {
        "id": "5b9058d6-af61-4e64-9ae5-f2c4493f2910"
      },
      "outputs": [],
      "source": [
        "samples = [\n",
        "    \"너무 재밌어요, 꿀잼 인정!\",\n",
        "    \"보다가 중간에 졸았습니다 ㅠㅠ 비추에요\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hqT9z9PqOnvV"
      },
      "id": "hqT9z9PqOnvV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ee7804e2-c961-45db-b4d8-76d17d42efa5",
      "metadata": {
        "id": "ee7804e2-c961-45db-b4d8-76d17d42efa5"
      },
      "source": [
        "### 저장\n",
        "\n",
        "잘 학습된 것을 확인했다면 파일에 저장하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JUhH2CupOojg"
      },
      "id": "JUhH2CupOojg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0b4a5e3a-7625-4eb5-a9bd-5036ef7fb759",
      "metadata": {
        "id": "0b4a5e3a-7625-4eb5-a9bd-5036ef7fb759"
      },
      "source": [
        "## 정리\n",
        "\n",
        "이번 챕터에서는 huggingface에서 제공하는 tokenizers 라이브러리를 이용해서 직접 subword tokenizer를 만들어보았습니다. subword tokenizer는 활용도가 매우 높으니, 사용법을 잘 기억해주시기 바랍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f577a249-06ad-4123-bbb6-5527d1ad1b0d",
      "metadata": {
        "id": "f577a249-06ad-4123-bbb6-5527d1ad1b0d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}