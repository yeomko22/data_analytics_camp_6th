{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb73a3ab-b76c-44c7-b1e2-afb5046bbf23",
   "metadata": {},
   "source": [
    "# ch1. subword tokenization\n",
    "\n",
    "이번 챕터에서는 subword 기반의 토큰화 기법에 대해서 알아보겠습니다. 지금까지 사용해본 토큰화 기법은 다음과 같은 종류가 있었습니다.\n",
    "\n",
    "- 공백 기준 토큰화\n",
    "- 단어 기준 토큰화\n",
    "- 형태소 기준 토큰화\n",
    "\n",
    "그러나 이는 신조어나 고유 명사들에 취약한 약점이 있었습니다. 이를 극복하기 위해 단어를 더 작은 단위인 subword로 나누어 토큰화 한 것이 subword tokenizer입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31df57dd-5c2e-48b5-b7c6-68c25f7a3b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /Users/user/miniconda3/lib/python3.10/site-packages (0.15.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /Users/user/miniconda3/lib/python3.10/site-packages (from tokenizers) (0.20.2)\n",
      "Requirement already satisfied: filelock in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.9.2)\n",
      "Requirement already satisfied: requests in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2022.12.7)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.8.3.post1 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: mecab-python 0.996-ko-0.9.2 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of mecab-python or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bba460a-f884-49f0-bf97-f570b8117520",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding\n",
    "\n",
    "\n",
    "### Byte Pair Encoding 개념\n",
    "BPE라고 줄여서 많이 표기합니다. 본래 데이터 압축 알고리즘으로, 데이터에서 가장 많이 등장한 문자열을 병합해서 압축하는 기법입니다.\n",
    "\n",
    "- aaabdaaabac\n",
    "\n",
    "위와 같은 문자열이 주어졌다고 가정하겠습니다. 단어에 등장한 글자로 사전을 구성하면 (a, b, c, d)가 됩니다. 이제 가장 많이 등장하는 연속하는 두 글자를 하나로 병합합니다. 여기서는 aa가 동시에 많이 등장하므로 이를 Z로 치환해보겠습니다.\n",
    "\n",
    "- ZabdZabac\n",
    "\n",
    "이제 글자 사전은 (a, b, c, d, Z)가 됩니다. 이 문자열에서 다시 ab를 Y로 압축해보겠습니다.\n",
    "\n",
    "- ZYdZYac\n",
    "\n",
    "사전에는 Y가 추가되어서 (a, b, c, d, Z, Y)가 됩니다. 마지막으로 ZY를 묶어서 X로 치환해보겠습니다.\n",
    "\n",
    "- XdXac\n",
    "\n",
    "최종 사전은 (a, b, c, d, Z, Y, X)가 됩니다. 정리해보면 글자 사전은 처음 4개에서 7개로 어휘가 3개 증가했지만, 문자열은 처음 길이 11에서 5로 줄었습니다. 즉, BPE는 사전 크기 증가는 억제하면서 정보를 효율적으로 압축할 수 있는 알고리즘입니다. \n",
    "\n",
    "BPE의 가장 큰 장점은 분석 대상 언어에 대한 사전 지식이 필요 없습니다. 그저 말뭉치 데이터만 주어지면 자주 나타나는 문자열을 토큰으로 분석하기 때문에 어떤 언어에도 적용이 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6f4324-f938-4f0a-9183-7a125f1bf74c",
   "metadata": {},
   "source": [
    "### BPE 토큰화 과정\n",
    "\n",
    "1. 어휘 집합 구축\n",
    "\n",
    "자주 등장하는 문자열을 병합하고 이를 어휘 집합에 추가합니다. 미리 지정한 어휘 집합의 크기가 될 때까지 반복합니다.\n",
    "\n",
    "2. 토큰화\n",
    "\n",
    "토큰화 대상 문장을 띄어쓰기로 분리하고, 어휘 집합에 있는 subword가 포함되어 있을 때 해당 subword를 각 어절에서 분리합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a88fb2-d190-42fd-8be9-65e7e5ce5624",
   "metadata": {},
   "source": [
    "## BPE 학습시키기 \n",
    "\n",
    "먼저 아래와 같이 말뭉치 데이터가 주어졌다고 가정하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef0a9ff-9a89-43d5-91ab-217cb21d38ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"this is the hugging face course .\",\n",
    "    \"this chapter is about tokenization .\",\n",
    "    \"this section shows several tokenizer algorithms .\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b62599d-95b0-4144-a91e-3dc7a63942d1",
   "metadata": {},
   "source": [
    "### Pre-tokenize & build vocab\n",
    "\n",
    "이를 공백을 기준으로 나눠준 뒤, 처음 vocab 사전을 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f917a14-9cc5-415b-bdf4-55c62b92b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for sentence in corpus:\n",
    "    words.extend(sentence.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b40cf34c-ebbe-4456-84e5-4329e7d91d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'hugging', 'face', 'course', '.', 'this', 'chapter', 'is', 'about', 'tokenization', '.', 'this', 'section', 'shows', 'several', 'tokenizer', 'algorithms', '.', 'Hopefully,', 'you', 'will', 'be', 'able', 'to', 'understand', 'how', 'they', 'are', 'trained', 'and', 'generate', 'tokens.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc41f2d-3f62-4f70-8e17-adbbb7061b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = set()\n",
    "for word in words:\n",
    "    for c in word:\n",
    "        vocabs.add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caac4279-e2f6-43f0-a31c-b11195644e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a', 'o', 'v', ',', 'w', 's', 'e', 'r', 'd', '.', 'c', 'n', 'u', 'z', 't', 'f', 'g', 'm', 'p', 'h', 'y', 'b', 'k', 'i', 'l', 'H'}\n"
     ]
    }
   ],
   "source": [
    "print(vocabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9843709b-20c6-481f-b332-baf643a1657a",
   "metadata": {},
   "source": [
    "### tokenize using vocab \n",
    "\n",
    "사전을 이용하여 각 단어들을 토큰화 하는 함수를 작성해보겠습니다. 이때, 단어를 vocab에 들어있는 토큰 단위로 쪼개기 위해서 정규표현식을 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14815969-f03a-403a-bde4-be0f9d20cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def build_pattern(vocabs):\n",
    "    sorted_vocabs = sorted(vocabs, key=lambda x: len(x), reverse=True)\n",
    "    pattern = re.compile(f\"|\".join(sorted_vocabs))\n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b6c79c-4d14-4967-bdb2-fa1c4f0a2351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('a|o|v|,|w|s|e|r|d|.|c|n|u|z|t|f|g|m|p|h|y|b|k|i|l|H')\n"
     ]
    }
   ],
   "source": [
    "pattern = build_pattern(vocabs)\n",
    "print(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b740c05-9fd5-4fa5-a718-ca19e7cf0737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(words, pattern):\n",
    "    tokenized = []\n",
    "    for word in words:\n",
    "        tokens = pattern.findall(word)\n",
    "        tokenized.append(tokens)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21ea0752-8e8f-49c1-bc54-f582bdc0198d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['t', 'h', 'i', 's'],\n",
       " ['i', 's'],\n",
       " ['t', 'h', 'e'],\n",
       " ['h', 'u', 'g', 'g', 'i', 'n', 'g'],\n",
       " ['f', 'a', 'c', 'e'],\n",
       " ['c', 'o', 'u', 'r', 's', 'e'],\n",
       " ['.'],\n",
       " ['t', 'h', 'i', 's'],\n",
       " ['c', 'h', 'a', 'p', 't', 'e', 'r'],\n",
       " ['i', 's'],\n",
       " ['a', 'b', 'o', 'u', 't'],\n",
       " ['t', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n'],\n",
       " ['.'],\n",
       " ['t', 'h', 'i', 's'],\n",
       " ['s', 'e', 'c', 't', 'i', 'o', 'n'],\n",
       " ['s', 'h', 'o', 'w', 's'],\n",
       " ['s', 'e', 'v', 'e', 'r', 'a', 'l'],\n",
       " ['t', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'],\n",
       " ['a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'],\n",
       " ['.'],\n",
       " ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y', ','],\n",
       " ['y', 'o', 'u'],\n",
       " ['w', 'i', 'l', 'l'],\n",
       " ['b', 'e'],\n",
       " ['a', 'b', 'l', 'e'],\n",
       " ['t', 'o'],\n",
       " ['u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'],\n",
       " ['h', 'o', 'w'],\n",
       " ['t', 'h', 'e', 'y'],\n",
       " ['a', 'r', 'e'],\n",
       " ['t', 'r', 'a', 'i', 'n', 'e', 'd'],\n",
       " ['a', 'n', 'd'],\n",
       " ['g', 'e', 'n', 'e', 'r', 'a', 't', 'e'],\n",
       " ['t', 'o', 'k', 'e', 'n', 's', '.']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(words, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eace8f5-b2b4-4feb-9dce-39b01192bde5",
   "metadata": {},
   "source": [
    "### Pair counting \n",
    "이제 어휘를 추가하기 위해서 함께 자주 등장하는 쌍을 찾아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fea5f73c-602f-4cb5-8e5c-2019933ebaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "\n",
    "def count_pairs(words, pattern):\n",
    "    c = Counter()\n",
    "    for word in words:\n",
    "        tokens = pattern.findall(word)\n",
    "        for i in range(len(tokens) - 1):\n",
    "            c[f\"{tokens[i]}{tokens[i+1]}\"] += 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2934798-a429-4ecb-bc42-fa9a1ebfe9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counter = count_pairs(words, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da0f979c-dd4e-48c4-9d0c-92abfc605692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'th': 6, 'is': 5, 'er': 5, 'to': 4, 'en': 4, 'hi': 3, 'ou': 3, 'se': 3, 'ok': 3, 'ke': 3, 'ra': 3, 'nd': 3, 'he': 2, 'in': 2, 'rs': 2, 'te': 2, 'ab': 2, 'ni': 2, 'iz': 2, 'at': 2, 'ti': 2, 'io': 2, 'on': 2, 'ho': 2, 'ow': 2, 'al': 2, 'll': 2, 'an': 2, 'ne': 2, 'hu': 1, 'ug': 1, 'gg': 1, 'gi': 1, 'ng': 1, 'fa': 1, 'ac': 1, 'ce': 1, 'co': 1, 'ur': 1, 'ch': 1, 'ha': 1, 'ap': 1, 'pt': 1, 'bo': 1, 'ut': 1, 'za': 1, 'ec': 1, 'ct': 1, 'sh': 1, 'ws': 1, 'ev': 1, 've': 1, 'ze': 1, 'lg': 1, 'go': 1, 'or': 1, 'ri': 1, 'it': 1, 'hm': 1, 'ms': 1, 'Ho': 1, 'op': 1, 'pe': 1, 'ef': 1, 'fu': 1, 'ul': 1, 'ly': 1, 'y,': 1, 'yo': 1, 'wi': 1, 'il': 1, 'be': 1, 'bl': 1, 'le': 1, 'un': 1, 'de': 1, 'st': 1, 'ta': 1, 'ey': 1, 'ar': 1, 're': 1, 'tr': 1, 'ai': 1, 'ed': 1, 'ge': 1, 'ns': 1, 's.': 1})\n"
     ]
    }
   ],
   "source": [
    "print(pair_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79ecd9c-121b-4524-a774-8008ae76eb59",
   "metadata": {},
   "source": [
    "### vocab 추가\n",
    "\n",
    "가장 많이 등장한 쌍을 사전에 추가하겠습니다. 그 다음 words를 다시 토큰화 하여 결과를 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e56dadb4-4e06-403e-afc1-bf4c94f49a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['th', 'i', 's'],\n",
       " ['i', 's'],\n",
       " ['th', 'e'],\n",
       " ['h', 'u', 'g', 'g', 'i', 'n', 'g'],\n",
       " ['f', 'a', 'c', 'e'],\n",
       " ['c', 'o', 'u', 'r', 's', 'e'],\n",
       " ['.'],\n",
       " ['th', 'i', 's'],\n",
       " ['c', 'h', 'a', 'p', 't', 'e', 'r'],\n",
       " ['i', 's'],\n",
       " ['a', 'b', 'o', 'u', 't'],\n",
       " ['t', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n'],\n",
       " ['.'],\n",
       " ['th', 'i', 's'],\n",
       " ['s', 'e', 'c', 't', 'i', 'o', 'n'],\n",
       " ['s', 'h', 'o', 'w', 's'],\n",
       " ['s', 'e', 'v', 'e', 'r', 'a', 'l'],\n",
       " ['t', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'],\n",
       " ['a', 'l', 'g', 'o', 'r', 'i', 'th', 'm', 's'],\n",
       " ['.'],\n",
       " ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y', ','],\n",
       " ['y', 'o', 'u'],\n",
       " ['w', 'i', 'l', 'l'],\n",
       " ['b', 'e'],\n",
       " ['a', 'b', 'l', 'e'],\n",
       " ['t', 'o'],\n",
       " ['u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'],\n",
       " ['h', 'o', 'w'],\n",
       " ['th', 'e', 'y'],\n",
       " ['a', 'r', 'e'],\n",
       " ['t', 'r', 'a', 'i', 'n', 'e', 'd'],\n",
       " ['a', 'n', 'd'],\n",
       " ['g', 'e', 'n', 'e', 'r', 'a', 't', 'e'],\n",
       " ['t', 'o', 'k', 'e', 'n', 's', '.']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_pair = pair_counter.most_common()[0][0]\n",
    "vocabs.add(most_common_pair)\n",
    "pattern = build_pattern(vocabs)\n",
    "tokenize(words, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee1197-94d1-41a9-b8ce-3db98d4c0b10",
   "metadata": {},
   "source": [
    "### 지정한 vocab 수 달성까지 반복 \n",
    "\n",
    "미리 지정한 vocab 수를 달성할 때까지 반복해줍니다. vocab 수가 50이 될 때까지 반복한 뒤에 토큰화 해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1e4ae31-865d-40a3-a18d-51e097926d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len vocabs: 27\n",
      "{'a', 'o', 'v', ',', 'w', 's', 'e', 'r', 'd', '.', 'c', 'n', 'u', 'z', 't', 'f', 'g', 'm', 'p', 'h', 'y', 'b', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 28\n",
      "{'a', 'o', 'v', ',', 'w', 's', 'is', 'e', 'r', 'd', '.', 'c', 'n', 'u', 'z', 't', 'f', 'g', 'm', 'p', 'h', 'y', 'b', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 29\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'is', 'e', 'r', 'd', '.', 'c', 'n', 'u', 'z', 't', 'f', 'g', 'm', 'p', 'h', 'y', 'b', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 30\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'is', 'e', 'r', 'd', '.', 'to', 'c', 'n', 'u', 'z', 't', 'f', 'g', 'm', 'p', 'h', 'y', 'b', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 31\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'is', 'e', 'r', 'd', '.', 'to', 'c', 'n', 'u', 'z', 't', 'f', 'g', 'en', 'm', 'p', 'h', 'y', 'b', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 32\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'is', 'e', 'r', 'd', '.', 'to', 'c', 'n', 'u', 'z', 't', 'f', 'g', 'en', 'm', 'p', 'this', 'h', 'y', 'b', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 33\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'is', 'e', 'r', 'd', '.', 'to', 'c', 'ou', 'n', 'u', 'z', 't', 'f', 'g', 'en', 'm', 'p', 'this', 'h', 'y', 'b', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 34\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'is', 'e', 'r', 'd', '.', 'to', 'c', 'ou', 'n', 'u', 'z', 't', 'f', 'g', 'en', 'm', 'p', 'this', 'h', 'y', 'se', 'b', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 35\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'is', 'e', 'r', 'd', '.', 'to', 'c', 'ou', 'n', 'u', 'z', 't', 'f', 'g', 'en', 'm', 'p', 'this', 'h', 'y', 'se', 'tok', 'b', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 36\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'is', 'e', 'r', 'd', '.', 'to', 'c', 'ou', 'n', 'u', 'z', 't', 'f', 'g', 'en', 'm', 'p', 'this', 'h', 'y', 'se', 'tok', 'token', 'b', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 37\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'is', 'e', 'r', 'd', '.', 'to', 'c', 'ou', 'n', 'u', 'z', 't', 'f', 'g', 'en', 'm', 'p', 'this', 'h', 'y', 'se', 'tok', 'token', 'b', 'nd', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 38\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'is', 'e', 'r', 'd', '.', 'to', 'c', 'ou', 'n', 'u', 'z', 't', 'f', 'g', 'the', 'en', 'm', 'p', 'this', 'h', 'y', 'se', 'tok', 'token', 'b', 'nd', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 39\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'in', 'is', 'e', 'r', 'd', '.', 'to', 'c', 'ou', 'n', 'u', 'z', 't', 'f', 'g', 'the', 'en', 'm', 'p', 'this', 'h', 'y', 'se', 'tok', 'token', 'b', 'nd', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 40\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'in', 'is', 'e', 'r', 'd', '.', 'to', 'c', 'ou', 'n', 'u', 'z', 't', 'f', 'g', 'the', 'en', 'm', 'p', 'this', 'h', 'y', 'se', 'tok', 'token', 'b', 'ab', 'nd', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 41\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'in', 'is', 'e', 'r', 'd', '.', 'to', 'c', 'ou', 'n', 'u', 'z', 't', 'f', 'g', 'the', 'en', 'm', 'p', 'this', 'h', 'y', 'se', 'tok', 'token', 'b', 'ab', 'nd', 'tokeni', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 42\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'in', 'tokeniz', 'is', 'e', 'r', 'd', '.', 'to', 'c', 'ou', 'n', 'u', 'z', 't', 'f', 'g', 'the', 'en', 'm', 'p', 'this', 'h', 'y', 'se', 'tok', 'token', 'b', 'ab', 'nd', 'tokeni', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 43\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'in', 'tokeniz', 'is', 'at', 'e', 'r', 'd', '.', 'to', 'c', 'ou', 'n', 'u', 'z', 't', 'f', 'g', 'the', 'en', 'm', 'p', 'this', 'h', 'y', 'se', 'tok', 'token', 'b', 'ab', 'nd', 'tokeni', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 44\n",
      "{'a', 'er', 'o', 'v', ',', 'w', 's', 'in', 'tokeniz', 'is', 'at', 'e', 'r', 'd', '.', 'to', 'c', 'ou', 'io', 'n', 'u', 'z', 't', 'f', 'g', 'the', 'en', 'm', 'p', 'this', 'h', 'y', 'se', 'tok', 'token', 'b', 'ab', 'nd', 'tokeni', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 45\n",
      "{'a', 'er', 'ion', 'o', 'v', ',', 'w', 's', 'in', 'tokeniz', 'is', 'at', 'e', 'r', 'd', '.', 'to', 'c', 'ou', 'io', 'n', 'u', 'z', 't', 'f', 'g', 'the', 'en', 'm', 'p', 'this', 'h', 'y', 'se', 'tok', 'token', 'b', 'ab', 'nd', 'tokeni', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 46\n",
      "{'a', 'er', 'ion', 'o', 'v', ',', 'w', 's', 'in', 'tokeniz', 'is', 'at', 'e', 'ho', 'r', 'd', '.', 'to', 'c', 'ou', 'io', 'n', 'u', 'z', 't', 'f', 'g', 'the', 'en', 'm', 'p', 'this', 'h', 'y', 'se', 'tok', 'token', 'b', 'ab', 'nd', 'tokeni', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 47\n",
      "{'a', 'er', 'ion', 'o', 'v', ',', 'w', 's', 'in', 'tokeniz', 'is', 'at', 'e', 'ho', 'r', 'd', '.', 'to', 'c', 'ou', 'io', 'n', 'u', 'z', 't', 'f', 'g', 'the', 'en', 'm', 'how', 'p', 'this', 'h', 'y', 'se', 'tok', 'token', 'b', 'ab', 'nd', 'tokeni', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 48\n",
      "{'a', 'er', 'ion', 'o', 'v', ',', 'w', 's', 'in', 'tokeniz', 'is', 'at', 'e', 'ho', 'r', 'd', '.', 'to', 'c', 'ou', 'io', 'n', 'u', 'z', 't', 'f', 'g', 'the', 'en', 'm', 'how', 'p', 'this', 'h', 'y', 'se', 'tok', 'token', 'b', 'ab', 'nd', 'al', 'tokeni', 'k', 'i', 'th', 'l', 'H'}\n",
      "len vocabs: 49\n",
      "{'a', 'er', 'ion', 'o', 'v', ',', 'w', 's', 'in', 'tokeniz', 'is', 'at', 'e', 'ho', 'r', 'd', '.', 'to', 'c', 'ou', 'io', 'n', 'u', 'z', 't', 'f', 'g', 'the', 'en', 'm', 'how', 'p', 'this', 'h', 'y', 'll', 'se', 'tok', 'token', 'b', 'ab', 'nd', 'al', 'tokeni', 'k', 'i', 'th', 'l', 'H'}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50\n",
    "while len(vocabs) < vocab_size:\n",
    "    print(f\"len vocabs: {len(vocabs)}\")\n",
    "    print(vocabs)\n",
    "    pattern = build_pattern(vocabs)\n",
    "    pair_counter = count_pairs(words, pattern)\n",
    "    most_common_pair = pair_counter.most_common()[0][0]\n",
    "    vocabs.add(most_common_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be80bd05-c6c6-49b3-bd51-dbb5c0b8c575",
   "metadata": {},
   "source": [
    "반복을 거듭할 수록 같이 자주 등장하는 토큰들이 vocabs에 추가되는 것을 확인할 수 있습니다. 이렇게 얻어낸 vocab으로 토큰화 하는 방식이 BPE tokenizer 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bcea45f-2bf6-4873-a361-57cdb83e252f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this'],\n",
       " ['is'],\n",
       " ['the'],\n",
       " ['h', 'u', 'g', 'g', 'in', 'g'],\n",
       " ['f', 'a', 'c', 'e'],\n",
       " ['c', 'ou', 'r', 'se'],\n",
       " ['.'],\n",
       " ['this'],\n",
       " ['c', 'h', 'a', 'p', 't', 'er'],\n",
       " ['is'],\n",
       " ['ab', 'ou', 't'],\n",
       " ['tokeniz', 'at', 'ion'],\n",
       " ['.'],\n",
       " ['this'],\n",
       " ['se', 'c', 't', 'ion'],\n",
       " ['s', 'how', 's'],\n",
       " ['se', 'v', 'er', 'al'],\n",
       " ['tokeniz', 'er'],\n",
       " ['al', 'g', 'o', 'r', 'i', 'th', 'm', 's'],\n",
       " ['.'],\n",
       " ['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y', ','],\n",
       " ['y', 'ou'],\n",
       " ['w', 'i', 'll'],\n",
       " ['b', 'e'],\n",
       " ['ab', 'l', 'e'],\n",
       " ['to'],\n",
       " ['u', 'nd', 'er', 's', 't', 'and'],\n",
       " ['how'],\n",
       " ['the', 'y'],\n",
       " ['a', 'r', 'e'],\n",
       " ['t', 'r', 'a', 'in', 'e', 'd'],\n",
       " ['and'],\n",
       " ['g', 'en', 'er', 'at', 'e'],\n",
       " ['token', 's', '.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = build_pattern(vocabs)\n",
    "tokenize(words, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4f2d2-c391-4795-a7d9-af18da73a9a0",
   "metadata": {},
   "source": [
    "### OOV 대응\n",
    "\n",
    "subword 기반의 토큰화 기법들은 신조어나 고유 명사에 강하다고 했습니다. 실제로 학습 과정에서 본 적 없는 단어가 주어졌을 때, 어떻게 토큰화 하는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0cb6cba-934d-4d51-84d1-8ced19f776d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"tokenization is the first step of natural language processing.\"\n",
    "new_words = new_sentence.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e4f56dc-04b8-4899-91af-3d91043d4bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tokeniz', 'at', 'ion'],\n",
       " ['is'],\n",
       " ['the'],\n",
       " ['f', 'i', 'r', 's', 't'],\n",
       " ['s', 't', 'e', 'p'],\n",
       " ['o', 'f'],\n",
       " ['n', 'at', 'u', 'r', 'al'],\n",
       " ['l', 'a', 'n', 'g', 'u', 'a', 'g', 'e'],\n",
       " ['p', 'r', 'o', 'c', 'e', 's', 's', 'in', 'g', '.']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(new_words, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f2055-e6a7-4399-a04c-2333d0f62d2a",
   "metadata": {},
   "source": [
    "이런 식으로 BPE tokenization은 한번도 본 적 없는 단어라 할 지라도 subword 단위로 쪼개주는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e33a0-178e-4d6a-82ee-f05d3018d641",
   "metadata": {},
   "source": [
    "## 그 외 subword tokenization 기법들\n",
    "\n",
    "BPE는 가장 기본적인 subword tokenization 기법입니다. 이 외에도 subword tokenization 기법들이 있습니다만, 핵심적인 개념은 비슷하므로 간단히 짚고 넘어가겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0229bde-f14b-47f1-8b44-09ac25e174b9",
   "metadata": {},
   "source": [
    "### WordPiece Tokenizer\n",
    "\n",
    "WordPiece Tokenizer은 BPE의 변형 알고리즘입니다. BPE가 빈도수에 기반하여 가장 많이 등장한 쌍을 병합하는 것과는 달리, 병합되었을 때 코퍼스의 우도(Likelihood)를 가장 높이는 쌍을 병합합니다. 이 기법은 구글의 language model인 BERT를 학습시킬 때 사용되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651f04ce-5d91-4de2-adf5-4fe58b41c754",
   "metadata": {},
   "source": [
    "### Unigram Language Model Tokenizer\n",
    "\n",
    "유니그램 언어 모델 토크나이저는 각각의 서브워드들에 대해서 손실(loss)을 계산합니다. 여기서 서브 단어의 손실이라는 것은 해당 서브워드가 단어 집합에서 제거되었을 경우, 코퍼스의 우도(Likelihood)가 감소하는 정도를 말합니다. 이렇게 측정된 서브워드들을 손실의 정도로 정렬하여, 최악의 영향을 주는 10~20%의 토큰을 제거합니다. 이를 원하는 단어 집합의 크기에 도달할 때까지 반복합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992b0d8-a778-4e1f-a528-6ff99d8902ce",
   "metadata": {},
   "source": [
    "### 어떤 기법을 선택할 것인가?\n",
    "\n",
    "subword tokenizer의 경우 corpus를 기반으로 집계한 vocab에 따라서 tokenize 결과가 달라집니다. 때문에 BERT나 llama와 같은 오픈 소스 language model의 경우, 자신들이 사용한 tokenizer를 함께 공개합니다. 이들 모델을 그대로 사용하고 싶거나, fine-tuning 하고 싶다면 이들이 공개한 tokenizer를 사용하시면 됩니다.\n",
    "\n",
    "그 외에 직접 tokenizer를 학습을 경우, 정답은 없습니다만 WordPiece Tokenizer가 선호됩니다. vocab size의 경우엔 corpus의 사이즈, 언어, 테스크의 종류에 따라서 달라질 수 있습니다. 작게 시작하고 서서히 늘려보는 것을 추천합니다. \n",
    "\n",
    "subword tokenizer에 대해서 추가적으로 궁금하신 분들은 아래 논문을 참고해주세요.  \n",
    "A COMPREHENSIVE ANALYSIS OF SUBWORD TOKENIZERS FOR\n",
    "MORPHOLOGICALLY RICH LANGUAGES: https://www.cmpe.boun.edu.tr/~gungort/theses/A%20Comprehensive%20Analysis%20of%20Subword%20Tokenizers%20for%20Morphologically%20Rich%20Languages.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32126a93-4830-4bc4-8939-c0ca2b7074eb",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "이번 챕터에서는 subword tokenization의 핵심 개념과 주요 기법들에 대해서 알아보았습니다. 다음 챕터에서는 huggingface 라이브러리를 이용해서 직접 subword tokenizer를 학습시키고 사용하는 방법에 대해서 알아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9e788-fef6-464a-aeb8-ba01cb2647d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
