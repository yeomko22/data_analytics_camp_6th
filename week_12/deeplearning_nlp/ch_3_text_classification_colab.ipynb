{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeomko22/data_analytics_camp_6th/blob/main/week_12/deeplearning_nlp/ch_3_text_classification_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb695b4-243d-4c55-aafe-87c18037534b",
      "metadata": {
        "id": "9bb695b4-243d-4c55-aafe-87c18037534b"
      },
      "source": [
        "# ch 3. text classification\n",
        "\n",
        "이미지와 마찬가지로 텍스트 데이터도 분류가 가장 기본적인 테스크입니다. 긍정적인 리뷰와 부정적인 리뷰로 분류하는 모델을 만들어보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a4aa6f1-d55c-4184-89fe-5c8a6adc1809",
      "metadata": {
        "id": "4a4aa6f1-d55c-4184-89fe-5c8a6adc1809"
      },
      "source": [
        "## 사전 작업"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d3479f5-45c2-4da2-b650-13e7a1d956aa",
      "metadata": {
        "id": "0d3479f5-45c2-4da2-b650-13e7a1d956aa"
      },
      "source": [
        "### 데이터 셋 준비"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GDrrbCLlO3E8"
      },
      "id": "GDrrbCLlO3E8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bc6bfaff-d68a-4ba5-889b-23c3a68a8042",
      "metadata": {
        "id": "bc6bfaff-d68a-4ba5-889b-23c3a68a8042"
      },
      "source": [
        "### tokenizer 준비"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_JatzkIRO4G8"
      },
      "id": "_JatzkIRO4G8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2ed7e558-d862-4fb3-9cd5-dd09f4e43211",
      "metadata": {
        "id": "2ed7e558-d862-4fb3-9cd5-dd09f4e43211"
      },
      "source": [
        "## 데이터 전처리\n",
        "\n",
        "텍스트 데이터 셋도 이미지 데이터 셋과 마찬가지로 dataset과 dataloader가 필요합니다. 그리고 텍스트를 텐서로 변환하는 전처리 로직도 필요합니다. 우선 간단하게 dataset과 dataloader를 만들어주겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22231f8f-ee71-414c-ab01-af50353270fb",
      "metadata": {
        "id": "22231f8f-ee71-414c-ab01-af50353270fb"
      },
      "source": [
        "### Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ArC-1OwSO5AG"
      },
      "id": "ArC-1OwSO5AG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "aea81e12-322a-4854-ae28-3427801c3b67",
      "metadata": {
        "id": "aea81e12-322a-4854-ae28-3427801c3b67"
      },
      "source": [
        "텍스트 데이터의 특징은 문장들 간에 길이가 다르다는 것입니다. 즉, 문장을 토큰화화면 길이가 달라집니다. 배치에 들어있는 텍스트 데이터를 토큰화 해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b2LhfXOuO6uo"
      },
      "id": "b2LhfXOuO6uo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "93a40905-9b1d-4420-a9af-0bbeacc664c2",
      "metadata": {
        "id": "93a40905-9b1d-4420-a9af-0bbeacc664c2"
      },
      "source": [
        "### Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8147dc4b-6155-4d8c-864a-0f94278cb26f",
      "metadata": {
        "id": "8147dc4b-6155-4d8c-864a-0f94278cb26f"
      },
      "source": [
        "위 예시에서 data loader가 batch size만큼 문장을 읽어왔습니다. 그리고 이를 토큰화 하였는데, 문장마다 길이가 다르기 때문에 토큰 개수도 다릅니다. 이를 하나의 텐서로 묶어주기 위해서는 가장 긴 문장의 길이를 기준으로 나머지 문장들에 패딩을 추가해주어야 합니다.\n",
        "\n",
        "먼저 패딩에 해당하는 토큰을 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Er8owN9LO9Ix"
      },
      "id": "Er8owN9LO9Ix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "76f0f6e7-44c3-428a-bf3f-a42c78c4b309",
      "metadata": {
        "id": "76f0f6e7-44c3-428a-bf3f-a42c78c4b309"
      },
      "source": [
        "torch에 내장되어 있는 pad_sequence 기능을 이용해서 패딩을 채워보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XDdM-nHKO-Hi"
      },
      "id": "XDdM-nHKO-Hi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bf92a9bb-4451-43f7-a36b-4c12dbd16548",
      "metadata": {
        "id": "bf92a9bb-4451-43f7-a36b-4c12dbd16548"
      },
      "source": [
        "패딩을 채워보면 하나의 열에 문장 하나의 토큰 값들이 담기게 됩니다. 가장 긴 문장의 길이만큼 나머지 문장들에는 패딩이 채워진 모습을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb9f5d89-ea7f-4d51-ad53-12e7e761a961",
      "metadata": {
        "id": "cb9f5d89-ea7f-4d51-ad53-12e7e761a961"
      },
      "source": [
        "### collate_fn\n",
        "\n",
        "이렇게 텍스트를 토큰화 하고, 배치 단위로 패딩을 채워넣었습니다. 그런데 패딩을 채우기 위해서는 배치 내에서 가장 길이가 긴 문장의 토큰 수를 알아야 합니다. 즉, 데이터 셋에서 데이터 로더가 배치 사이즈만큼 데이터를 가져오는 시점에 텍스트를 토큰화하고, 패딩을 채우는 작업을 수행해주어야 합니다. 이러한 작업을 수행해주는 역할을 묶어주는 함수, collate_fn이라고 부릅니다.\n",
        "\n",
        "collate 함수 안에서는 데이터를 토큰화 해주고, 패딩을 채워주는 작업을 진행합니다. 이 때, 특정 문장의 길이가 지나치게 긴 경우를 예방하기 위해 최대 토큰 길이 값을 지정해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gHzqhlaoO_Yy"
      },
      "id": "gHzqhlaoO_Yy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5573c9fd-d6be-4c83-94a8-a67931768b6d",
      "metadata": {
        "id": "5573c9fd-d6be-4c83-94a8-a67931768b6d"
      },
      "source": [
        "## 모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0b95c5-d611-45d8-a3ba-986f31e2470b",
      "metadata": {
        "id": "4d0b95c5-d611-45d8-a3ba-986f31e2470b"
      },
      "source": [
        "### 모델 작성\n",
        "\n",
        "이제 텍스트를 분류하는 딥러닝 모델을 만들어보겠습니다. 모델의 구조는 아래와 같습니다.\n",
        "\n",
        "![Untitled Diagram-Page-4.drawio.png](https://storage.googleapis.com/data-analytics-camp/week12_deeplearning_nlp/1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb833bc2-3ebb-427f-a3be-78f0d220dd0e",
      "metadata": {
        "id": "fb833bc2-3ebb-427f-a3be-78f0d220dd0e"
      },
      "source": [
        "먼저 토큰화 한 문장을 embedding layer를 통과시켜 각 토큰별 임베딩 벡터를 가져옵니다. 각 토큰별로 128 차원의 임베딩을 갖도록 하고, 현재 tokenizer는 총 8000개의 토큰을 가지고 있으므로 임베딩 레이어는 8000 X 128이 됩니다. 이제 문장에 포함된 토큰만 가져온 뒤, 평균을 냅니다. 그리고 이를 리니어 레이어를 하나 통과시킨 뒤, 학습을 시켜보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V8oROUO9PBcN"
      },
      "id": "V8oROUO9PBcN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c1720f90-c657-42b5-b7db-17bead2da615",
      "metadata": {
        "id": "c1720f90-c657-42b5-b7db-17bead2da615"
      },
      "source": [
        "### 학습 코드 준비\n",
        "\n",
        "기존 모델 학습에 사용했던 코드를 재사용하여 학습을 진행해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b24033a2-a949-4a15-b08d-b6411336253e",
      "metadata": {
        "id": "b24033a2-a949-4a15-b08d-b6411336253e"
      },
      "outputs": [],
      "source": [
        "def get_mean(metrics):\n",
        "    return round(sum(metrics) / len(metrics), 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2df9dc3b-a0bd-4538-9b71-5c4e4f33449b",
      "metadata": {
        "id": "2df9dc3b-a0bd-4538-9b71-5c4e4f33449b"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def train_model(model):\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    acc_list = []\n",
        "    for x_train, y_train in tqdm(train_dataloader):\n",
        "        x_train = x_train.to(device)\n",
        "        y_train = y_train.to(device)\n",
        "\n",
        "        outputs = model(x_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(outputs, dim=1)\n",
        "        acc = ((y_train == pred).sum() / len(y_train)).item()\n",
        "        acc_list.append(acc)\n",
        "    return get_mean(loss_list), get_mean(acc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cb67439-1a6b-4db3-859d-f325d4ba2ce7",
      "metadata": {
        "id": "4cb67439-1a6b-4db3-859d-f325d4ba2ce7"
      },
      "outputs": [],
      "source": [
        "def validate_model(model):\n",
        "    model.eval()\n",
        "    loss_list = []\n",
        "    acc_list = []\n",
        "    for x_val, y_val in tqdm(val_dataloader):\n",
        "        x_val = x_val.to(device)\n",
        "        y_val = y_val.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(x_val)\n",
        "            loss = criterion(outputs, y_val)\n",
        "            loss_list.append(loss.item())\n",
        "\n",
        "            pred = torch.argmax(outputs, dim=1)\n",
        "            acc = ((y_val == pred).sum() / len(y_val)).item()\n",
        "            acc_list.append(acc)\n",
        "    return get_mean(loss_list), get_mean(acc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9b9800d-48c9-46a0-aff3-2cc19a537ad7",
      "metadata": {
        "id": "d9b9800d-48c9-46a0-aff3-2cc19a537ad7"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def train_validate_model(model):\n",
        "    logs = defaultdict(list)\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = train_model(model)\n",
        "        val_loss, val_acc = validate_model(model)\n",
        "        logs[\"train_loss\"].append(train_loss)\n",
        "        logs[\"train_acc\"].append(train_acc)\n",
        "        logs[\"val_loss\"].append(val_loss)\n",
        "        logs[\"val_acc\"].append(val_acc)\n",
        "        print(f\"epoch {epoch + 1} train - loss: {train_loss} acc: {train_acc} val - loss: {val_loss} acc: {val_acc}\")\n",
        "    return logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d95cc1a1-15dc-4fdc-b3eb-fcb8743c183e",
      "metadata": {
        "id": "d95cc1a1-15dc-4fdc-b3eb-fcb8743c183e"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "def plot_logs(logs):\n",
        "    fig = plt.figure(figsize=(10, 4))\n",
        "\n",
        "    ax0 = fig.add_subplot(1, 2, 1)\n",
        "    ax1 = fig.add_subplot(1, 2, 2)\n",
        "    ax0.plot(logs[\"train_loss\"], label=\"train\")\n",
        "    ax0.plot(logs[\"val_loss\"], label=\"val\")\n",
        "    ax0.legend()\n",
        "    ax0.set_title(\"loss\")\n",
        "\n",
        "    ax1.plot(logs[\"train_acc\"], label=\"train\")\n",
        "    ax1.plot(logs[\"val_acc\"], label=\"val\")\n",
        "    ax1.legend()\n",
        "    ax1.set_title(\"accuracy\")\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8db04c2-370f-4afb-9299-11e513e5ddab",
      "metadata": {
        "id": "f8db04c2-370f-4afb-9299-11e513e5ddab"
      },
      "source": [
        "### 하이퍼 파라미터 셋팅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "515f081c-86b8-41b9-bbe2-74cce9e767df",
      "metadata": {
        "id": "515f081c-86b8-41b9-bbe2-74cce9e767df"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "learning_rate = 0.001\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "epochs = 10\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcb56424-69fb-4a30-b290-e04138270281",
      "metadata": {
        "id": "bcb56424-69fb-4a30-b290-e04138270281"
      },
      "source": [
        "### 학습"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R7DmDgSUPEfW"
      },
      "id": "R7DmDgSUPEfW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ea19fdd9-6fa7-4a81-88df-d0106041f511",
      "metadata": {
        "id": "ea19fdd9-6fa7-4a81-88df-d0106041f511"
      },
      "source": [
        "## 정리\n",
        "\n",
        "이번 챕터에서는 텍스트 데이터를 DataLoader를 이용해서 텐서로 변환하고, Embedding Layer와 Fully Connected Layer를 이용해서 분류 모델을 학습시키는 작업을 수행해보았습니다. 그 결과 84% 정도의 성능을 얻을 수 있었습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aadd3af7-15f8-436c-bd1c-d70bfc76845a",
      "metadata": {
        "id": "aadd3af7-15f8-436c-bd1c-d70bfc76845a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}