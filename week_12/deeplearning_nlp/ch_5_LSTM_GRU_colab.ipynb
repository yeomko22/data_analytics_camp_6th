{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeomko22/data_analytics_camp_6th/blob/main/week_12/deeplearning_nlp/ch_5_LSTM_GRU_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOXnOpGpbShY"
      },
      "source": [
        "# ch 5. LSTM\n",
        "\n",
        "이전 챕터에서는 가장 기본적인 형태의 RNN에 대해서 배워봤습니다.  하지만 바닐라 RNN은 바닐라 RNN의 시점(time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생합니다. 즉, 시퀀스가 길어질 수록 과거의 정보가 손실되는 문제가 있습니다.\n",
        "\n",
        "이를 보완하는 모델이 LSTM입니다. 이는 장기 기억으로 필요한 정보는 유지하고, 불필요한 기억은 삭제할 수 있는 게이트라는 개념을 RNN에 추가한 것입니다. 장기 기억을 효과적으로 보존하여 자연어 처리와 시계열 분석에 널리 사용됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWXrSVE8f6VJ"
      },
      "source": [
        "## LSTM 이론"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhBiBKxGbVC2"
      },
      "source": [
        "### LSTM 기본 구조\n",
        "\n",
        "LSTM은 vanila RNN과 기본 구조는 비슷합니다. 각 시점 별로 입력 값에 대해서 예측값과 hidden state를 계산하여 다음 상태에 전달합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGvZYE9Fm-KM"
      },
      "source": [
        "![image](https://storage.googleapis.com/data-analytics-camp/week12_deeplearning_nlp/10.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzIY36-5npA7"
      },
      "source": [
        "이제 LSTM의 내부 구조를 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nxRTX4zntZ-"
      },
      "source": [
        "![image](https://storage.googleapis.com/data-analytics-camp/week12_deeplearning_nlp/11.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqnbk7ZOoBxI"
      },
      "source": [
        "![image](https://storage.googleapis.com/data-analytics-camp/week12_deeplearning_nlp/12.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Poiss_hGnxRn"
      },
      "source": [
        "매우 복잡하게 생겼습니다만, 걱정할 필요가 없습니다. 일일이 암기할 필요는 없고, 핵심적인 개념만 숙지하겠습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GNZHb_6sAgR"
      },
      "source": [
        "### Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtGkxoMbr9UM"
      },
      "source": [
        "![image](https://storage.googleapis.com/data-analytics-camp/week12_deeplearning_nlp/13.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LFu9Rybr0sV"
      },
      "source": [
        "먼저 LSTM은 3가지 게이트를 이용해서 어떤 정보를 망각하고, 기억할 지를 결정합니다. 위 그림에서 𝜎로 표기된 것은 sigmoid 함수입니다. 총 3개의 sigmoid 함수를 사용하고 각각은 \"망각\", \"기억\", \"출력\"을 의마합니다. 즉, LSTM은 어떤 정보를 잊어버리고, 어떤 정보는 기억할 지를 이 Gate를 이용하여 학습시킵니다. 각 게이트별 세부 내용은 아래서 설명하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsDpzIpMoI26"
      },
      "source": [
        "### Cell State\n",
        "\n",
        "먼저 LSTM은 다음 시점에 cell state와 hidden state 2가지 값을 전달합니다. 먼저 cell state는 이전 시점으로부터 상태 값을 받아와서 \"망각\"과 \"기억\" 과정을 거쳐서 다음 상태로 전달되는 값입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E1xzDR2qXVQ"
      },
      "source": [
        "![image](https://storage.googleapis.com/data-analytics-camp/week12_deeplearning_nlp/14.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF4acM44sx2p"
      },
      "source": [
        "### forget gate\n",
        "\n",
        "먼저 망각 게이트입니다. 이는 현재 시점의 입력과 이전 시점의 hidden state를 입력으로 받아서 linear layer와 sigmoid를 통과시켜줍니다. sigmoid를 통과했으니 값이 0과 1 사이가 됩니다. 이를 이전 시점의 cell state에 곱해주면, 어떤 정보는 잊고 어떤 정보는 보존할 지 결정하는 망각을 구현할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2R6DWJgtD0A"
      },
      "source": [
        "![image](https://storage.googleapis.com/data-analytics-camp/week12_deeplearning_nlp/15.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNi7FTcptz8d"
      },
      "source": [
        "### input gate\n",
        "\n",
        "그 다음 cell state에 현재 시점의 입력 정보를 추가하는 input gate입니다. 이는 현재 시점의 값 중 어떤 것을 cell state에 추가할 지 게이트를 거친 다음, 걸러낸 정보를 cell state에 더해줍니다. 즉, cell state에 현재 입력을 바탕으로 계산한 새로운 정보를 추가해주는 입력을 구현하게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rah_DCPvt60W"
      },
      "source": [
        "![image](https://storage.googleapis.com/data-analytics-camp/week12_deeplearning_nlp/16.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KJr9kqmp8lQ"
      },
      "source": [
        "### output gate & hidden state\n",
        "\n",
        "마지막으로 출력 게이트입니다. 이는 새롭게 구한 cell state와 현재 시점의 입력, 이전 시점의 hidden state로 계산하여 새로운 hidden state를 구합니다. 그리고 이를 출력으로 내주고, 다음 시점으로 전달해줍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10rYl2l24XcX"
      },
      "source": [
        "![image](https://storage.googleapis.com/data-analytics-camp/week12_deeplearning_nlp/17.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AnPh_Em4v9m"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w487TaiNfrSX"
      },
      "source": [
        "## 실습 사전 작업\n",
        "\n",
        "이제 LSTM을 이용해서 문장 분류 모델을 학습시켜 보겠습니다. 데이터 셋을 불러오고, 토크나이저와 데이터 셋, 데이터 로더를 작성하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJje6_c8if8O"
      },
      "source": [
        "### 데이터 셋 준비"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wluJGzFfQsUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_LrsZApkHuv"
      },
      "source": [
        "### tokenizer 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnAHgVhbkLct",
        "outputId": "637052f4-b90e-47bd-d013-cbe824abcdc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in /Users/user/miniconda3/lib/python3.10/site-packages (0.15.0)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /Users/user/miniconda3/lib/python3.10/site-packages (from tokenizers) (0.20.2)\n",
            "Requirement already satisfied: filelock in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.9.2)\n",
            "Requirement already satisfied: requests in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/user/miniconda3/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/user/miniconda3/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2022.12.7)\n",
            "\u001b[33mDEPRECATION: pytorch-lightning 1.8.3.post1 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: mecab-python 0.996-ko-0.9.2 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of mecab-python or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t7YvhjjIQtWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHhDkqzNkTC7"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWN_gHkUkGQ1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, corpus_df, transform=None):\n",
        "        self.corpus_df = corpus_df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.corpus_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text, label = self.corpus_df.iloc[idx]\n",
        "        return text, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7R6WihKkaij"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomTextDataset(train_df)\n",
        "val_dataset = CustomTextDataset(val_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7C2En_zs8i-"
      },
      "source": [
        "### DataLoader\n",
        "\n",
        "dataloader 구현 시에 배치 내 각 문장별 토큰 개수를 집계한 batch_lengths를 추가로 리턴해주도록 합니다. 이는 RNN 학습 시에 pack_padded_sequence를 사용하기 위함이입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp_y6g3JkcCd"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS = 256\n",
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppR8whfbkpRL"
      },
      "outputs": [],
      "source": [
        "vocabs = tokenizer.get_vocab()\n",
        "pad_token = vocabs[\"[PAD]\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyDIXfBLkfy1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def _tokenize(text):\n",
        "    tokens = tokenizer.encode(text).ids\n",
        "    tokens = tokens[:MAX_TOKENS]\n",
        "    token_tensor = torch.tensor(tokens, dtype=torch.long)\n",
        "    return token_tensor\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch_text = [x[0] for x in batch]\n",
        "    batch_label = torch.tensor([x[1] for x in batch], dtype=torch.long)\n",
        "    batch_tokens = [_tokenize(x) for x in batch_text]\n",
        "    batch_lengths = torch.tensor([len(x) for x in batch_tokens])\n",
        "    batch_padded = pad_sequence(batch_tokens, padding_value=pad_token)\n",
        "    return batch_padded, batch_label, batch_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwJufBpckhCz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkBbOftaky9M"
      },
      "outputs": [],
      "source": [
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4Ffu7l0kuKQ",
        "outputId": "355e5f86-3ce8-461c-af27-726c7896558e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[1042, 1300, 6037,  ..., 5145, 2283, 9393],\n",
            "        [2705, 2660, 6429,  ..., 3265, 2669, 7752],\n",
            "        [2966, 7116,    7,  ..., 1107, 2636, 5110],\n",
            "        ...,\n",
            "        [   0,    0,    0,  ...,    0,    0,    0],\n",
            "        [   0,    0,    0,  ...,    0,    0,    0],\n",
            "        [   0,    0,    0,  ...,    0,    0,    0]]), tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
            "        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
            "        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
            "        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0]), tensor([34, 13, 56, 19, 11, 15,  5,  3,  9, 29,  5, 21,  7, 32, 16, 24, 10, 13,\n",
            "        10, 10,  9, 45, 14, 17,  4, 11,  4, 21,  6,  6,  2, 14,  7,  9,  7,  6,\n",
            "         6, 18, 10, 25, 14,  2, 15,  7, 11,  8, 56, 14, 42,  5, 42,  8,  3, 15,\n",
            "        12, 18, 26, 12, 14, 11, 21, 13, 12, 11, 17, 34,  8,  1, 10, 28, 21, 13,\n",
            "        33,  6, 23,  9, 23,  3, 16, 32,  2,  4, 65, 18,  6, 12, 12, 11,  9, 13,\n",
            "        11, 27, 17,  7,  7,  9, 15, 10, 15,  9,  9,  2, 50, 19,  6,  9,  5,  3,\n",
            "         3, 23, 12, 15, 10, 31,  3, 20, 14,  6, 58, 15, 13, 23, 31, 16, 22,  7,\n",
            "         5, 40]))\n"
          ]
        }
      ],
      "source": [
        "train_iterator = iter(train_dataloader)\n",
        "batch = next(train_iterator)\n",
        "print(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh_eoD_ik-0b"
      },
      "source": [
        "## LSTM 문장 분류 모델 학습\n",
        "\n",
        "이제 RNN을 이용해서 문장을 분류하는 모델을 만들고, 이를 학습시켜보겠습니다. LSTM도 RNN과 마찬가지로 상태 값을 양방향으로 흐르도록 설정할 수 있습니다. 그리고 여러 층을 쌓을 수 있습니다. 한번 bidirectional=True, num_layers=2로 설정해서 모델을 학습시켜 보겠습니다.\n",
        "\n",
        "전반적인 구조는 아래와 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-JJvnxTBtl9"
      },
      "source": [
        "![image](https://storage.googleapis.com/data-analytics-camp/week12_deeplearning_nlp/18.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47LowjsglaoH"
      },
      "source": [
        "### 모델 작성"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ljRB8kzdQvui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hxL5LPUpaHm"
      },
      "source": [
        "### 학습 코드 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F2vNYfrpXHx"
      },
      "outputs": [],
      "source": [
        "def get_mean(metrics):\n",
        "    return round(sum(metrics) / len(metrics), 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_r0I1Hepda7"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def train_model(model):\n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    acc_list = []\n",
        "    for x_train, y_train, lengths in tqdm(train_dataloader):\n",
        "        x_train = x_train.to(device)\n",
        "        y_train = y_train.to(device)\n",
        "\n",
        "        outputs = model(x_train, lengths)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(outputs, dim=1)\n",
        "        acc = ((y_train == pred).sum() / len(y_train)).item()\n",
        "        acc_list.append(acc)\n",
        "    return get_mean(loss_list), get_mean(acc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSvqySNbpesx"
      },
      "outputs": [],
      "source": [
        "def validate_model(model):\n",
        "    model.eval()\n",
        "    loss_list = []\n",
        "    acc_list = []\n",
        "    for x_val, y_val, lengths in tqdm(val_dataloader):\n",
        "        x_val = x_val.to(device)\n",
        "        y_val = y_val.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(x_val, lengths)\n",
        "            loss = criterion(outputs, y_val)\n",
        "            loss_list.append(loss.item())\n",
        "\n",
        "            pred = torch.argmax(outputs, dim=1)\n",
        "            acc = ((y_val == pred).sum() / len(y_val)).item()\n",
        "            acc_list.append(acc)\n",
        "    return get_mean(loss_list), get_mean(acc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOnvI5N-pgJA"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def train_validate_model(model):\n",
        "    logs = defaultdict(list)\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = train_model(model)\n",
        "        val_loss, val_acc = validate_model(model)\n",
        "        logs[\"train_loss\"].append(train_loss)\n",
        "        logs[\"train_acc\"].append(train_acc)\n",
        "        logs[\"val_loss\"].append(val_loss)\n",
        "        logs[\"val_acc\"].append(val_acc)\n",
        "        print(f\"epoch {epoch + 1} train - loss: {train_loss} acc: {train_acc} val - loss: {val_loss} acc: {val_acc}\")\n",
        "    return logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IwLAPPCpire"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_logs(logs):\n",
        "    fig = plt.figure(figsize=(10, 4))\n",
        "\n",
        "    ax0 = fig.add_subplot(1, 2, 1)\n",
        "    ax1 = fig.add_subplot(1, 2, 2)\n",
        "    ax0.plot(logs[\"train_loss\"], label=\"train\")\n",
        "    ax0.plot(logs[\"val_loss\"], label=\"val\")\n",
        "    ax0.legend()\n",
        "    ax0.set_title(\"loss\")\n",
        "\n",
        "    ax1.plot(logs[\"train_acc\"], label=\"train\")\n",
        "    ax1.plot(logs[\"val_acc\"], label=\"val\")\n",
        "    ax1.legend()\n",
        "    ax1.set_title(\"accuracy\")\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zno3lRi3pmBz"
      },
      "source": [
        "### 하이퍼 파라미터 셋팅"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5OYkFRMGQzU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGkXmg0Fpp3C"
      },
      "source": [
        "### 학습"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W4wfUAlaQ0WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyq9ZZFd9OzP"
      },
      "source": [
        "## GRU를 이용한 문장 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "052iwV467iWc"
      },
      "source": [
        "\n",
        "\n",
        "### GRU 기본 구조\n",
        "Gated Recurrent Unit의 약자로, LSTM의 약점을 보완한 모델입니다. LSTM은 다음 셀로 cell state와 hidden_state 2가지 상태 값을 넘겨주었습니다. GRU는 이것이 불필요하다고 판단하여 제거하고, hidden_state 하나만 넘겨주는 간소화 된 모델입니다.\n",
        "\n",
        "계산 수식은 몹시 복잡하지만, 모두 기억하지 않아도 됩니다. LSTM을 간소화 하였지만 성능은 비슷하거나 오히려 더 뛰어난 모델이라는 것만 짚고 넘어가겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yCfpQ8V8y-Y"
      },
      "source": [
        "![image](https://storage.googleapis.com/data-analytics-camp/week12_deeplearning_nlp/19.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkXNlM4r97GI"
      },
      "source": [
        "### GRU 모델 작성 및 학습\n",
        "\n",
        "GRU는 다음 셀로 hidden_state 하나만 넘겨주면 되므로, vanila RNN과 코드 구현이 거의 동일합니다. 기존 RNN을 GRU로만 바꿔주면 됩니다. 구조는 아래와 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECaxL1CFCM0q"
      },
      "source": [
        "![image](https://storage.googleapis.com/data-analytics-camp/week12_deeplearning_nlp/20.png)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yc3dlkYfQ17y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYnWVOBM-c1k"
      },
      "source": [
        "GRU도 마찬가지로 bidirectional=True, num_layers는 2로 설정하여 학습시켜 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-irUie3Q27o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5Sj193aODBJ"
      },
      "source": [
        "## 정리\n",
        "\n",
        "이번 챕터에서는 장기 기억을 저장할 수 있는 LSTM 모델에 대해서 알아보았습니다. 특히나 긴 sequence를 다룰 때에는 vanila RNN 보다 LSTM 혹은 GRU를 많이 사용합니다. 구조가 많이 복잡했지만, 실제 사용하는 방법은 어렵지 않았습니다.\n",
        "\n",
        "너무 수식 하나하나에 매달리지 말고, 장기 기억을 저장하기 위한 구조라는 점만 기억하고 넘어가겠습니다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}